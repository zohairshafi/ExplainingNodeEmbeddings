{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247831ec",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05843082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm\n",
    "from DGI.models import DGI, LogReg\n",
    "from DGI.utils import process\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "\n",
    "from scripts.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8a58e1",
   "metadata": {},
   "source": [
    "### Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da42c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbedder:\n",
    "    def __init__(self, graph, embed_shape = (128,)):\n",
    "        self.embed(graph)\n",
    "        self.E = list(graph.edges())\n",
    "        self.graph = graph\n",
    "        self.embed_shape = embed_shape\n",
    "    \n",
    "    def embed(self, graph):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        raise NotImplementedError\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce27d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56d64531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGIEmbedding(BaseEmbedder):\n",
    "    def __init__(self, embed_dim = 64, graph = None, feature_matrix = None, use_xm = False, debug = False, batch_size = 1, nb_epochs = 2500, patience = 20, ortho_ = 0.1, sparse_ = 0.1, lr = 1e-3, l2_coef = 0.0, drop_prob = 0.0, sparse = True, nonlinearity = 'prelu'):\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Training Params\n",
    "        self.graph = graph\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.patience = patience\n",
    "        self.lr = lr\n",
    "        self.l2_coef = l2_coef\n",
    "        self.feature_matrix = feature_matrix\n",
    "        self.drop_prob = drop_prob\n",
    "        self.hid_units = embed_dim\n",
    "        self.sparse = sparse\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.use_xm = use_xm\n",
    "        self.ortho_ = ortho_\n",
    "        self.sparse_ = sparse_\n",
    "        \n",
    "        if graph is not None:\n",
    "            self.embed()\n",
    "        else:\n",
    "            self.graph = None\n",
    "    \n",
    "    def embed(self):\n",
    "\n",
    "        \n",
    "        if self.feature_matrix is None:\n",
    "            feature_matrix = np.identity(len(self.graph))\n",
    "        else: \n",
    "            feature_matrix = self.feature_matrix\n",
    "\n",
    "        adj = nx.to_scipy_sparse_array(self.graph)\n",
    "        features = sp.lil_matrix(feature_matrix)\n",
    "        features, _ = process.preprocess_features(features)\n",
    "\n",
    "        nb_nodes = features.shape[0]\n",
    "        ft_size = features.shape[1]\n",
    "\n",
    "        adj = process.normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "        if self.sparse:\n",
    "            sp_adj = process.sparse_mx_to_torch_sparse_tensor(adj)\n",
    "        else:\n",
    "            adj = (adj + sp.eye(adj.shape[0])).todense()\n",
    "\n",
    "        features = torch.FloatTensor(features[np.newaxis])\n",
    "        if not self.sparse:\n",
    "            adj = torch.FloatTensor(adj[np.newaxis])\n",
    "\n",
    "        if self.feature_matrix is not None: \n",
    "            sense_features = torch.FloatTensor(self.feature_matrix)\n",
    "\n",
    "\n",
    "        model = DGI(ft_size, self.hid_units, self.nonlinearity)\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr = self.lr, weight_decay = self.l2_coef)\n",
    "\n",
    "        b_xent = nn.BCEWithLogitsLoss()\n",
    "        xent = nn.CrossEntropyLoss()\n",
    "        cnt_wait = 0\n",
    "        best = 1e9\n",
    "        best_t = 0\n",
    "\n",
    "        for epoch in tqdm(range(self.nb_epochs)):\n",
    "            model.train()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            idx = np.random.permutation(nb_nodes)\n",
    "            shuf_fts = features[:, idx, :]\n",
    "\n",
    "            lbl_1 = torch.ones(self.batch_size, nb_nodes)\n",
    "            lbl_2 = torch.zeros(self.batch_size, nb_nodes)\n",
    "            lbl = torch.cat((lbl_1, lbl_2), 1)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                shuf_fts = shuf_fts.cuda()\n",
    "                lbl = lbl.cuda()\n",
    "\n",
    "            logits = model(features, shuf_fts, sp_adj if self.sparse else adj, self.sparse, None, None, None) \n",
    "            print (logits.shape)\n",
    "\n",
    "            if self.use_xm == True and feature_matrix is not None:\n",
    "                sf = sense_features[idx, :]\n",
    "                sense_mat = torch.einsum('ij, ik -> ijk', logits, sf)\n",
    "                E = sense_mat\n",
    "                y_norm = torch.diagonal(torch.matmul(logits, torch.transpose(logits, 0, 1)))\n",
    "                sense_norm = torch.diagonal(torch.matmul(sf, torch.transpose(sf, 0, 1)))\n",
    "                norm = torch.multiply(y_norm, sense_norm)\n",
    "                print (E.shape)\n",
    "                print (norm.shape)\n",
    "                E = torch.transpose(torch.transpose(E, 0, 1) / norm)\n",
    "\n",
    "                ortho_loss = (self.ortho_ * E) / self.batch_size\n",
    "                sparse_loss = (self.sparse_ * torch.sum(torch.linalg.norm(E, ord = 1, axis = 0))) / batch_size\n",
    "\n",
    "            loss = b_xent(logits, lbl) + ortho_loss + sparse_loss\n",
    "\n",
    "            if self.debug:\n",
    "                print('Loss:', loss)\n",
    "\n",
    "            if loss < best:\n",
    "                best = loss\n",
    "                best_t = epoch\n",
    "                cnt_wait = 0\n",
    "                torch.save(model.state_dict(), 'best_dgi.pkl')\n",
    "            else:\n",
    "                cnt_wait += 1\n",
    "\n",
    "            if cnt_wait == self.patience:\n",
    "                if self.debug: \n",
    "                    print('Early stopping!')\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "        if self.debug: \n",
    "            print('Loading {}th epoch'.format(best_t))\n",
    "        model.load_state_dict(torch.load('best_dgi.pkl'))\n",
    "\n",
    "        self.node_model = model\n",
    "        self.fitted = True\n",
    "\n",
    "        embeds, _ = model.embed(features, sp_adj if self.sparse else adj, self.sparse, None)\n",
    "        self.embeddings = embeds\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        return np.squeeze(self.embeddings.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e0b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/email.pkl', 'rb') as file: \n",
    "    graph_dict = pkl.load(file)\n",
    "    \n",
    "graph = nx.Graph(nx.to_numpy_array(graph_dict['graph']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f776188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1972])\n",
      "torch.Size([986, 1972, 7])\n",
      "torch.Size([986])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7) must match the size of tensor b (986) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dgi \u001b[38;5;241m=\u001b[39m \u001b[43mDGIEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                   \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mfeature_matrix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msense_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                   \u001b[49m\u001b[43muse_xm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mortho_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msparse_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [36], line 24\u001b[0m, in \u001b[0;36mDGIEmbedding.__init__\u001b[0;34m(self, embed_dim, graph, feature_matrix, use_xm, debug, batch_size, nb_epochs, patience, ortho_, sparse_, lr, l2_coef, drop_prob, sparse, nonlinearity)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_ \u001b[38;5;241m=\u001b[39m sparse_\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [36], line 94\u001b[0m, in \u001b[0;36mDGIEmbedding.embed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m (E\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m (norm\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 94\u001b[0m E \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m)\n\u001b[1;32m     96\u001b[0m ortho_loss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mortho_ \u001b[38;5;241m*\u001b[39m E) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m     97\u001b[0m sparse_loss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_ \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(E, \u001b[38;5;28mord\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m))) \u001b[38;5;241m/\u001b[39m batch_size\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (7) must match the size of tensor b (986) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "dgi = DGIEmbedding(graph = graph, \n",
    "                   embed_dim = 64, \n",
    "                   feature_matrix = sense_features, \n",
    "                   use_xm = True, \n",
    "                   ortho_ = 0.1, \n",
    "                   sparse_ = 0.1, \n",
    "                   batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f002199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Degrees...                                   \r",
      "Calculating Average Neighbor Degree...                    \r",
      "Calculating Clustering Coefficient...                     \r",
      "Calculating Average Neighbor Clustering Coefficients...   \r",
      "Calculating Eccentricity...                               \r",
      "Calculating Page Rank...                                  \r",
      "Calculating Personalized Page Rank...                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "986it [01:45,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done                                                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zohairshafi/miniforge3/lib/python3.10/site-packages/networkx/algorithms/centrality/katz.py:325: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A = nx.adjacency_matrix(G, nodelist=nodelist, weight=weight).todense().T\n"
     ]
    }
   ],
   "source": [
    "sense_feat_dict, sense_features = get_sense_features(graph, ppr_flag = 'std')\n",
    "\n",
    "uncorrelated_feats = ['Degree',\n",
    "                    'Clustering Coefficient',\n",
    "                    'Personalized Page Rank - Standard Deviation',\n",
    "                    'Average Neighbor Degree',\n",
    "                    'Average Neighbor Clustering',\n",
    "                    'Eccentricity',\n",
    "                    'Katz Centrality']\n",
    "sense_features = sense_features[:, [list(sense_feat_dict).index(feat) for feat in uncorrelated_feats]]\n",
    "sense_feat_dict = {feat : idx for idx, feat in enumerate(uncorrelated_feats)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b281857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6181, 0.5628, 0.9938, 0.9559, 0.8618, 1.0928, 0.7465, 0.8630, 0.8639,\n",
       "        0.8429, 0.7087, 0.7559, 0.6545, 0.6687, 0.5673, 0.8514, 0.5724, 0.6970,\n",
       "        0.4402, 0.7604, 0.7440, 0.7943, 0.7221, 0.8261, 1.0566, 0.9244, 0.9343,\n",
       "        0.8780, 0.8233, 0.8003, 0.7912, 0.7670, 1.0185, 0.9083, 1.2444, 0.6826,\n",
       "        0.7661, 0.7836, 0.7537, 0.8212, 0.8221, 0.6622, 0.6772, 1.0986, 0.8488,\n",
       "        0.8046, 0.8970, 0.7833, 0.8579, 0.8335, 0.8521, 0.5424, 0.4901, 0.4613,\n",
       "        1.0689, 0.9767, 1.1368, 0.9257, 0.8808, 1.1523, 0.8627, 0.5037, 0.8086,\n",
       "        0.7742, 0.5804, 0.9541, 0.9653, 0.8088, 0.7450, 0.8030, 0.7846, 1.0215,\n",
       "        0.7370, 0.5899, 0.4515, 0.8545, 1.0824, 0.8220, 0.8494, 0.6706, 0.7349,\n",
       "        0.8424, 0.9342, 0.8012, 0.5575, 0.3935, 0.7924, 0.6692, 1.0691, 1.1735,\n",
       "        0.7741, 0.8809, 0.5204, 0.7094, 0.7348, 0.6425, 1.1050, 1.0988, 0.8956,\n",
       "        0.9687, 0.8292, 0.9124, 0.9144, 0.6828, 0.9563, 0.7152, 0.7443, 0.9620,\n",
       "        0.9270, 0.6514, 1.2519, 1.2761, 0.7523, 0.5580, 0.7708, 0.6649, 0.6939,\n",
       "        0.6885, 0.5501, 0.8537, 0.6055, 1.1116, 0.5766, 0.5923, 0.8903, 1.0515,\n",
       "        0.8882, 0.5670, 0.7205, 0.6936, 0.6854, 0.7720, 0.8849, 0.8549, 0.6195,\n",
       "        0.8691, 1.1805, 0.9163, 1.1100, 0.8766, 0.9569, 0.6610, 0.8332, 0.6157,\n",
       "        1.0399, 0.7282, 0.5190, 0.6966, 0.5718, 0.6519, 0.7542, 0.8143, 0.8930,\n",
       "        0.9596, 0.9346, 0.7625, 0.6039, 0.6643, 0.7967, 0.8973, 1.5896, 0.7380,\n",
       "        0.5683, 0.8011, 0.6896, 0.8180, 1.3851, 0.9702, 0.4993, 0.7252, 0.6520,\n",
       "        0.7698, 0.5491, 0.6994, 0.8192, 0.9789, 0.6201, 0.5677, 0.5679, 0.5913,\n",
       "        0.4341, 0.8137, 0.8360, 0.7659, 0.7457, 1.0877, 0.8717, 0.8335, 0.7607,\n",
       "        0.8319, 0.7948, 0.6389, 1.2513, 1.3810, 1.0741, 1.1514, 0.8171, 0.7346,\n",
       "        0.6580, 0.6542, 0.9290, 0.6464, 0.7652, 0.7237, 0.8636, 0.7535, 0.7239,\n",
       "        0.7353, 0.8062, 0.9416, 0.8854, 1.4088, 1.0534, 0.6747, 0.5876, 0.6159,\n",
       "        0.9252, 0.7729, 0.6290, 0.7432, 0.6080, 0.5312, 0.6159, 0.5849, 0.6418,\n",
       "        0.6908, 0.6972, 0.5358, 0.6472, 0.5798, 1.0089, 0.5649, 0.6388, 0.8354,\n",
       "        0.8743, 0.9207, 0.8551, 0.8832, 0.6513, 0.7435, 0.9944, 0.9316, 0.9223,\n",
       "        0.6312, 0.7520, 0.7263, 0.7426, 1.0118, 0.6528, 0.8272, 0.8204, 0.8639,\n",
       "        1.1012, 0.7670, 0.7218, 0.6272, 0.6629, 0.7447, 0.6962, 0.6468, 0.6798,\n",
       "        0.8895, 0.5161, 0.6842, 0.8070, 0.7842, 0.6395, 0.9288, 0.6493, 0.9668,\n",
       "        0.7208, 0.8166, 0.6874, 0.9434, 0.9370, 0.6802, 0.5979, 0.7881, 0.5244,\n",
       "        1.1034, 0.7004, 0.8436, 0.9616, 1.0331, 0.7060, 0.8754, 0.9590, 0.8082,\n",
       "        0.8776, 0.5252, 0.6939, 0.8819, 0.5362, 0.9918, 0.9074, 0.6384, 0.8310,\n",
       "        0.7545, 0.6447, 0.7778, 0.7455, 0.8393, 0.8594, 1.0107, 0.9901, 0.7628,\n",
       "        0.5782, 0.4975, 0.8398, 0.4422, 0.5312, 0.7344, 0.6148, 0.8491, 0.6226,\n",
       "        0.6539, 0.5358, 0.4882, 0.6442, 1.4435, 0.8488, 0.8332, 1.1736, 0.8196,\n",
       "        0.6840, 0.7289, 0.5974, 0.6718, 0.8336, 0.7960, 0.6726, 0.7276, 0.8800,\n",
       "        0.8824, 1.4078, 1.2105, 0.7328, 0.8738, 0.7281, 0.8954, 1.1844, 0.9103,\n",
       "        0.5829, 0.6209, 0.7306, 0.5782, 0.5781, 0.4671, 1.4631, 0.5408, 0.5815,\n",
       "        0.7351, 0.5839, 0.8933, 1.4003, 1.1716, 0.7704, 0.7832, 0.9156, 0.7885,\n",
       "        0.6748, 0.9607, 0.7292, 0.9860, 1.0077, 0.5386, 0.4846, 0.8586, 0.6442,\n",
       "        1.3879, 0.8337, 0.8925, 0.7139, 0.5145, 1.2003, 0.9581, 0.8173, 1.5219,\n",
       "        1.0897, 0.8101, 0.8996, 0.7051, 1.5470, 1.7300, 0.8992, 1.1862, 0.9693,\n",
       "        0.6156, 0.9945, 0.6247, 0.8279, 1.0264, 1.0657, 0.6927, 0.9625, 0.6951,\n",
       "        1.3535, 0.7593, 1.5466, 0.8274, 0.6792, 0.8885, 0.6651, 0.6788, 0.8498,\n",
       "        1.0265, 1.0578, 0.9578, 1.1628, 0.6160, 1.1713, 1.1169, 0.9054, 0.8252,\n",
       "        1.3822, 0.3880, 0.6091, 0.7952, 0.8076, 0.4687, 1.4852, 0.3848, 0.6517,\n",
       "        0.9555, 0.6667, 0.8634, 0.6867, 0.7948, 1.0710, 1.0752, 0.7259, 0.8593,\n",
       "        1.4653, 0.7148, 0.8408, 0.5610, 1.6510, 0.9515, 0.9218, 1.6119, 0.5472,\n",
       "        0.6889, 0.8341, 0.7759, 0.9364, 0.7157, 1.1808, 0.8736, 0.8332, 0.6417,\n",
       "        0.8372, 0.7847, 0.9320, 0.9101, 1.1808, 0.8563, 2.0005, 0.5488, 0.7150,\n",
       "        0.8143, 0.4335, 1.0176, 1.0034, 1.1079, 0.8819, 0.5358, 0.5805, 0.8943,\n",
       "        0.6259, 1.2100, 0.8711, 0.6056, 0.7376, 0.5969, 0.9812, 0.8868, 1.3504,\n",
       "        1.4818, 0.7445, 0.6452, 0.8804, 0.8964, 0.6259, 0.6112, 0.6983, 0.6428,\n",
       "        0.4655, 0.6996, 0.9360, 0.6315, 0.7216, 0.7299, 0.8687, 0.6276, 1.0306,\n",
       "        0.6317, 0.6597, 0.6596, 0.5420, 0.8045, 1.1442, 1.0101, 1.2337, 1.1510,\n",
       "        0.9193, 0.9858, 0.9782, 0.4838, 0.8089, 0.8907, 0.9337, 0.5213, 0.7256,\n",
       "        0.8339, 1.2379, 0.6823, 1.2419, 1.3477, 1.0763, 0.6374, 0.9350, 0.7053,\n",
       "        0.9429, 0.5831, 1.0262, 0.7760, 0.7630, 1.0411, 0.6242, 0.8177, 0.6225,\n",
       "        0.8376, 0.8607, 0.6130, 0.9361, 0.7853, 0.6013, 0.3682, 0.6674, 0.8380,\n",
       "        0.8990, 0.8074, 0.9188, 0.7403, 0.7538, 0.9776, 0.8545, 1.1534, 0.7107,\n",
       "        0.6389, 0.9680, 0.7657, 0.9578, 1.1197, 0.8175, 0.7536, 0.6838, 0.8454,\n",
       "        0.6891, 1.1422, 0.8497, 0.8653, 0.7349, 0.6203, 0.6214, 0.7991, 0.9100,\n",
       "        0.9205, 0.9208, 0.8243, 1.1110, 1.0486, 0.6891, 1.1180, 0.9752, 0.9839,\n",
       "        0.8723, 0.9000, 0.7431, 1.1226, 0.7631, 0.7995, 0.7669, 1.2285, 0.6305,\n",
       "        0.9754, 1.0276, 1.0013, 0.8814, 0.9092, 1.0692, 1.0321, 0.8205, 1.5778,\n",
       "        0.4031, 0.8213, 0.6349, 1.1777, 0.8902, 0.9426, 0.6246, 0.6506, 0.6417,\n",
       "        0.9558, 0.7849, 0.5873, 0.5844, 0.7826, 1.2077, 0.9078, 0.7246, 0.7478,\n",
       "        0.5466, 0.5787, 0.5218, 0.5804, 1.4285, 1.0074, 1.1835, 2.0148, 0.6671,\n",
       "        0.7830, 0.6421, 0.4848, 1.9697, 0.7152, 0.6924, 1.8142, 0.7139, 2.8240,\n",
       "        2.4997, 2.1446, 1.9506, 1.4852, 0.8272, 2.2809, 0.7015, 0.7469, 0.9225,\n",
       "        1.0336, 0.9393, 0.7115, 0.5576, 0.6555, 1.9240, 0.8831, 0.6608, 0.5180,\n",
       "        0.7329, 0.6835, 1.0093, 0.6549, 0.8071, 0.9577, 0.7991, 1.0579, 0.9671,\n",
       "        0.9336, 0.5939, 0.9361, 0.9677, 0.6885, 0.5155, 0.6672, 0.7761, 0.9973,\n",
       "        0.7520, 0.9802, 1.2939, 1.6515, 0.7316, 0.7494, 0.6794, 0.5752, 0.9241,\n",
       "        1.6616, 0.9505, 0.7074, 1.0646, 0.8443, 0.8694, 0.8544, 0.4995, 1.8027,\n",
       "        0.5506, 0.6045, 1.0373, 0.4922, 1.3526, 0.9174, 0.8205, 0.8653, 1.7472,\n",
       "        0.8501, 0.9113, 0.4457, 0.6708, 1.0542, 1.9760, 0.8018, 0.5434, 1.9166,\n",
       "        0.8054, 0.4949, 0.8069, 0.6903, 1.4375, 0.9065, 0.8242, 1.5943, 1.0726,\n",
       "        0.6994, 1.9936, 1.1833, 0.8468, 0.5388, 0.8139, 0.6551, 0.6216, 0.6690,\n",
       "        0.6405, 1.9392, 1.4146, 1.0925, 1.1631, 0.7289, 0.7380, 1.6491, 0.5948,\n",
       "        2.6895, 0.6361, 0.8398, 0.6096, 0.8723, 1.0408, 1.2212, 1.0010, 0.8913,\n",
       "        0.7190, 0.7679, 0.6277, 0.8504, 0.5354, 0.7349, 0.9281, 0.5793, 1.3524,\n",
       "        0.8069, 0.5998, 0.8745, 1.8397, 0.9627, 1.4096, 0.8886, 0.8621, 0.9936,\n",
       "        1.1227, 0.9432, 0.7340, 0.9348, 0.6486, 0.6722, 0.5746, 0.5458, 0.5582,\n",
       "        1.7150, 0.7777, 1.0941, 0.9958, 0.6341, 0.6998, 1.1565, 0.5292, 0.9208,\n",
       "        0.6674, 1.1554, 0.4422, 2.3314, 0.6701, 1.3415, 0.5326, 0.6631, 0.5349,\n",
       "        1.5965, 1.2866, 1.3111, 0.9058, 0.8975, 2.2479, 1.1393, 0.9750, 0.5656,\n",
       "        0.9474, 0.7108, 0.8180, 0.5620, 1.0247, 0.8919, 0.8895, 0.7625, 1.1238,\n",
       "        0.5605, 1.9962, 0.5447, 0.8947, 0.4593, 1.6090, 1.3631, 0.8689, 0.7825,\n",
       "        0.7663, 1.0231, 0.4560, 1.2051, 1.2653, 1.0457, 0.7613, 0.7175, 0.8501,\n",
       "        1.8972, 0.5422, 0.9730, 0.8222, 1.7131, 1.5305, 1.0250, 1.1144, 1.8963,\n",
       "        1.0132, 1.5605, 0.7767, 0.5626, 0.7222, 0.4832, 1.1256, 1.3640, 0.6082,\n",
       "        0.8468, 1.1908, 0.8334, 0.9240, 1.0593, 0.5551, 0.7750, 1.0503, 0.8917,\n",
       "        1.7655, 1.0396, 0.8440, 0.6156, 1.1792, 1.4862, 0.4439, 0.6063, 1.8014,\n",
       "        1.4862, 0.8897, 0.7220, 0.8647, 1.7257, 1.0327, 1.0601, 0.6061, 2.3717,\n",
       "        1.3491, 1.0885, 1.0679, 0.9528, 1.2451, 0.9439, 1.3806, 0.8949, 1.9600,\n",
       "        0.9168, 1.4514, 0.8720, 1.6833, 0.8168, 0.6248, 1.0530, 0.9499, 0.7942,\n",
       "        0.9912, 0.7534, 1.5618, 0.6159, 1.2480, 0.8353, 0.8792, 1.0142, 1.1564,\n",
       "        0.4916, 1.5909, 0.8835, 0.7282, 0.8876, 0.6564, 0.6417, 0.9583, 1.1748,\n",
       "        1.2901, 0.9491, 1.1215, 0.5870, 1.6241, 1.1527, 1.3124, 1.1505, 1.8442,\n",
       "        0.7679, 1.4603, 0.8874, 0.9756, 0.6968, 1.3622, 0.6868, 1.1099, 0.6898,\n",
       "        1.3616, 0.9536, 0.6374, 1.6277, 0.5087, 0.4992, 0.4860, 0.5851, 1.1160,\n",
       "        1.0126, 1.7817, 0.6731, 1.2209, 0.3920, 1.0121, 1.3195, 0.8833, 1.4063,\n",
       "        1.1863, 0.8343, 1.0039, 0.8718, 0.6486, 0.6486, 0.6486, 0.9013, 0.7482,\n",
       "        0.9638, 0.5109, 1.5993, 1.0557, 2.4156, 1.8996, 0.7950, 1.1619, 0.9165,\n",
       "        0.7853, 1.3510, 0.7853, 1.3323, 2.4793, 1.7062, 0.5260, 0.7520, 1.3323,\n",
       "        0.9912, 1.6493, 0.8052, 0.7070, 1.0716, 1.2138, 1.9156, 2.5187, 1.2227,\n",
       "        1.9544, 0.7904, 1.0507, 0.5337, 0.6412, 0.5340, 1.6153, 0.4916, 0.7004,\n",
       "        1.0105, 1.6155, 0.7340, 0.4560, 1.2707])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf = torch.FloatTensor(sense_features)\n",
    "torch.diagonal(torch.matmul(sf, torch.transpose(sf, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10581eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c5a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f44ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527055f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
