{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52338f1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd40674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import igraph\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import NMF, non_negative_factorization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "from IPython.display import Image\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "from tensorflow.keras import layers, models, Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, Dropout, Concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96f54fc",
   "metadata": {},
   "source": [
    "### Graph Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c742d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weights(G, weights):\n",
    "\n",
    "    '''\n",
    "        Add weights to a graph\n",
    "        Input : \n",
    "            G       : nx Graph object - Input graph\n",
    "            weights : String - Poisson | Uniform\n",
    "    '''\n",
    "\n",
    "    num_weights = G.number_of_edges()\n",
    "    \n",
    "    if weights == 'Poisson':\n",
    "        w = 1 + np.random.poisson(20, (num_weights))\n",
    "    elif weights == 'Uniform':\n",
    "        w = 1 + np.random.randint(41, size = (num_weights))\n",
    "    else:\n",
    "        w = np.ones((num_weights))\n",
    "\n",
    "    for idx, e in enumerate(G.edges):\n",
    "        G.edges[e]['weight'] = w[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3df02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(graph_type, num_nodes, param, weights):\n",
    "    \n",
    "    assert(weights in ['Poisson', 'Uniform', 'Equal'])\n",
    "\n",
    "    # Erdos-Renyi Graph\n",
    "    if graph_type == 'er':    \n",
    "        graph = nx.erdos_renyi_graph(n = num_nodes, p = param[0])\n",
    "        add_weights(graph, weights)\n",
    "\n",
    "    # Barabasi-Albert Graph\n",
    "    elif graph_type == 'ba':   \n",
    "        graph = nx.barabasi_albert_graph(n = num_nodes, m = param[0])\n",
    "        add_weights(graph, weights)\n",
    "        \n",
    "    # Watts-Strogatz Graph\n",
    "    elif graph_type == 'ws':   \n",
    "        graph = nx.watts_strogatz_graph(n = num_nodes, k = param[0], p = param[1])\n",
    "        add_weights(graph, weights)\n",
    "        \n",
    "    # Lattice Graph\n",
    "    elif graph_type == 'lattice':   \n",
    "        graph = nx.Graph(nx.adjacency_matrix(nx.grid_2d_graph(num_nodes, num_nodes)))\n",
    "        add_weights(graph, weights)\n",
    "    \n",
    "    # Complete Graph\n",
    "    elif graph_type == 'complete':\n",
    "        graph = nx.complete_graph(num_nodes)\n",
    "        add_weights(graph, weights)\n",
    "\n",
    "    else:\n",
    "        print('Invalid graph name. Please try one of : er, ba, ws, lattice, complete')\n",
    "        raise\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc5b81",
   "metadata": {},
   "source": [
    "### Sense Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77d4d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sense_features(graph, ppr_flag = 'std', weighted = False):\n",
    "    \n",
    "    if weighted: \n",
    "        sense_feat_dict = {\n",
    "\n",
    "            'Degree' : 0,\n",
    "            'Weighted Degree' : 1, \n",
    "            'Clustering Coefficient' : 2, \n",
    "            'Personalized Page Rank - Median' : 3,\n",
    "            'Personalized Page Rank - Standard Deviation' : 4,\n",
    "            'Structural Holes Constraint' : 5, \n",
    "            'Average Neighbor Degree' : 6,\n",
    "            'EgoNet Edges' : 7, \n",
    "            'Average Neighbor Clustering' : 8,\n",
    "            'Node Betweenness' : 9, \n",
    "            'Page Rank' : 10, \n",
    "            'Eccentricity' : 11,\n",
    "            'Degree Centrality' : 12, \n",
    "            'Eigen Centrality' : 13,\n",
    "            'Katz Centrality' : 14\n",
    "        }\n",
    "        \n",
    "    else: \n",
    "        sense_feat_dict = {\n",
    "\n",
    "            'Degree' : 0,\n",
    "            'Clustering Coefficient' : 1, \n",
    "            'Personalized Page Rank - Median' : 2,\n",
    "            'Personalized Page Rank - Standard Deviation' : 3,\n",
    "            'Structural Holes Constraint' : 4, \n",
    "            'Average Neighbor Degree' : 5,\n",
    "            'EgoNet Edges' : 6, \n",
    "            'Average Neighbor Clustering' : 7,\n",
    "            'Node Betweenness' : 8, \n",
    "            'Page Rank' : 9, \n",
    "            'Eccentricity' : 10,\n",
    "            'Degree Centrality' : 11, \n",
    "            'Eigen Centrality' : 12,\n",
    "            'Katz Centrality' : 13,\n",
    "        }\n",
    "    \n",
    "    if len(list(nx.algorithms.components.connected_components(graph))) > 1:\n",
    "        print (\"Disconnected Network\")\n",
    "        sense_feat_dict = {\n",
    "\n",
    "            'Degree' : 0,\n",
    "            'Weighted Degree' : 1, \n",
    "            'Clustering Coefficient' : 2, \n",
    "            'Personalized Page Rank - Median' : 3,\n",
    "            'Personalized Page Rank - Standard Deviation' : 4,\n",
    "            'Structural Holes Constraint' : 5, \n",
    "            'Average Neighbor Degree' : 6,\n",
    "            'EgoNet Edges' : 7, \n",
    "            'Average Neighbor Clustering' : 8,\n",
    "            'Node Betweenness' : 9, \n",
    "            'Page Rank' : 10, \n",
    "            'Degree Centrality' : 11, \n",
    "            'Eigen Centrality' : 12, \n",
    "            'Katz Centrality' : 13\n",
    "          }\n",
    "        \n",
    "    if ppr_flag == 'mean': \n",
    "        print (\"Using Means For PPR\")\n",
    "        sense_feat_dict = {\n",
    "    \n",
    "            'Degree' : 0,\n",
    "            'Weighted Degree' : 1, \n",
    "            'Clustering Coefficient' : 2, \n",
    "            'Personalized Page Rank - Mean' : 3,\n",
    "            'Structural Holes Constraint' : 4, \n",
    "            'Average Neighbor Degree' : 5,\n",
    "            'EgoNet Edges' : 6, \n",
    "            'Average Neighbor Clustering' : 7,\n",
    "            'Node Betweenness' : 8, \n",
    "            'Page Rank' : 9, \n",
    "            'Eccentricity' : 10,\n",
    "            'Degree Centrality' : 11,\n",
    "            'Eigen Centrality' : 12, \n",
    "            'Katz Centrality' : 13\n",
    "        }\n",
    "    \n",
    "    ig = igraph.Graph([[e[0], e[1]] for e in nx.to_edgelist(graph)])\n",
    "    sense_features = np.zeros((len(graph), len(sense_feat_dict)))\n",
    "\n",
    "    print (\"Calculating Degrees...                                   \", end = '\\r')\n",
    "    # Degree\n",
    "    sense_features[:, sense_feat_dict['Degree']] = list(dict(graph.degree).values())\n",
    "\n",
    "    if weighted: \n",
    "        print (\"Calculating Weighted Degrees...                           \", end = '\\r')\n",
    "        # Weighted Degree\n",
    "        sense_features[:, sense_feat_dict['Weighted Degree']] = list(dict(graph.degree(weight = 'weight')).values())\n",
    "    \n",
    "    print (\"Calculating Average Neighbor Degree...                    \", end = '\\r')\n",
    "    # Neighbor Degree Average\n",
    "    sense_features[:, sense_feat_dict['Average Neighbor Degree']] = [np.mean([graph.degree[neighbor] for neighbor in dict(graph[node]).keys()]) for node in graph.nodes]\n",
    "\n",
    "    print (\"Calculating Clustering Coefficient...                     \", end = '\\r')\n",
    "    # Clustering Coefficient\n",
    "    cluster_dict = nx.clustering(graph)\n",
    "    sense_features[:, sense_feat_dict['Clustering Coefficient']] = list(cluster_dict.values())\n",
    "\n",
    "    print (\"Calculating Average Neighbor Clustering Coefficients...   \", end = '\\r')\n",
    "    # Neighbor Average Clustering \n",
    "    sense_features[:, sense_feat_dict['Average Neighbor Clustering']] = [np.mean([cluster_dict[neighbor] for neighbor in list(graph[node])]) for node in graph.nodes]\n",
    "    \n",
    "    print (\"Calculating Eccentricity...                               \", end = '\\r')\n",
    "    # Eccentricity\n",
    "    try:\n",
    "        sense_features[:, sense_feat_dict['Eccentricity']] = ig.eccentricity() #list(nx.algorithms.distance_measures.eccentricity(graph).values())\n",
    "    except Exception as e: \n",
    "        print (\"Could not compute Eccentricity : \", e)\n",
    "    \n",
    "    print (\"Calculating Page Rank...                                  \", end = '\\r')\n",
    "    # Page Rank\n",
    "    sense_features[:, sense_feat_dict['Page Rank']] = ig.pagerank(directed = False) #list(nx.pagerank(graph).values())\n",
    "    \n",
    "    print (\"Calculating Personalized Page Rank...                     \", end = '\\r')\n",
    "    \n",
    "    if ppr_flag == 'mean':\n",
    "        ppr = np.zeros((1, len(graph)))\n",
    "        for node_idx, node in tqdm(enumerate(range(len(graph)))):\n",
    "            r = np.zeros((len(graph)))\n",
    "            r[node] = 1\n",
    "            ppr = ppr + ig.personalized_pagerank(reset = r, directed = False)\n",
    "        ppr = ppr / len(graph)\n",
    "        sense_features[:, sense_feat_dict['Personalized Page Rank - Mean']] = ppr\n",
    "        \n",
    "        \n",
    "    else: \n",
    "        ppr = np.zeros((len(graph), len(graph)))\n",
    "        for node_idx, node in tqdm(enumerate(range(len(graph)))):\n",
    "            r = np.zeros((len(graph)))\n",
    "            r[node] = 1\n",
    "            ppr[node_idx, :] = ig.personalized_pagerank(reset = r, directed = False)\n",
    "\n",
    "        sense_features[:, sense_feat_dict['Personalized Page Rank - Standard Deviation']] = np.std(ppr, axis = 0)\n",
    "        sense_features[:, sense_feat_dict['Personalized Page Rank - Median']] = np.median(ppr, axis = 0)\n",
    "    \n",
    "    print (\"Calculating Node Betweenness...                           \", end = '\\r')\n",
    "    # Node Betweenness \n",
    "    sense_features[:, sense_feat_dict['Node Betweenness']] = ig.betweenness(directed = False) #list(nx.algorithms.centrality.betweenness_centrality(graph).values())\n",
    "\n",
    "    print (\"Calculating Number Of Edges In Ego Nets...                \", end = '\\r')\n",
    "    # EgoNet Edges\n",
    "    sense_features[:, sense_feat_dict['EgoNet Edges']] = [len(nx.ego_graph(graph, n = node).edges) for node in graph.nodes]\n",
    "\n",
    "    print (\"Calculating Structural Hole Constraint Scores...         \", end = '\\r')\n",
    "    # Structual Holes\n",
    "    sense_features[:, sense_feat_dict['Structural Holes Constraint']] = ig.constraint() #list(nx.algorithms.structuralholes.constraint(graph, weight = 'weight').values())\n",
    "\n",
    "    \n",
    "    print (\"Calculating Degree Centrality...                         \", end = '\\r')\n",
    "    sense_features[:, sense_feat_dict['Degree Centrality']] =  list(dict(nx.degree_centrality(graph)).values())\n",
    "    \n",
    "    print (\"Calculating Eigen Centrality...                          \", end = '\\r')\n",
    "    sense_features[:, sense_feat_dict['Eigen Centrality']] = ig.eigenvector_centrality(directed = False)\n",
    "    \n",
    "    print (\"Calculating Katz Centrality...                           \", end = '\\r')\n",
    "    sense_features[:, sense_feat_dict['Katz Centrality']] =  list(dict(nx.katz_centrality_numpy(graph)).values())\n",
    "    \n",
    "    \n",
    "    print (\"Normalizing Features Between 0 And 1...                   \", end = '\\r')\n",
    "    # Normalise to between 0 and 1 \n",
    "    sense_features = (sense_features - np.min(sense_features, axis = 0)) / np.ptp(sense_features, axis = 0)\n",
    "    \n",
    "    print (\"Done                                                      \", end = '\\r')\n",
    "    \n",
    "    return sense_feat_dict, sense_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c9a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_sense_features(graph, num_anchors, anchor_list = None):\n",
    "        \n",
    "    graph.remove_edges_from(nx.selfloop_edges(graph))\n",
    "    \n",
    "    core_numbers = np.array(list(dict(nx.core_number(graph)).values()))\n",
    "    core_p = core_numbers / np.sum(core_numbers)\n",
    "    \n",
    "    if anchor_list is None:\n",
    "        core_anchors = np.random.choice(len(graph), p = core_p, size = num_anchors)\n",
    "    else: \n",
    "        core_anchors = anchor_list\n",
    "    \n",
    "    sense_feat_dict = []\n",
    "\n",
    "    sense_features = np.zeros((len(graph), 1 + (2 * num_anchors)))\n",
    "\n",
    "    ig = igraph.Graph([[e[0], e[1]] for e in nx.to_edgelist(graph)])\n",
    "\n",
    "    print (\"Computing Core Number...\", end = '\\r')\n",
    "    sense_features[:, len(sense_feat_dict)] = core_numbers\n",
    "    sense_feat_dict.append(\"Core Number\")\n",
    "\n",
    "    print (\"Computing PPR to Core Random Nodes...\", end = '\\r')\n",
    "    for idx, node in tqdm(enumerate(core_anchors)):\n",
    "        r = np.zeros((len(graph)))\n",
    "        r[node] = 1\n",
    "        sense_features[:, len(sense_feat_dict)] = ig.personalized_pagerank(reset = r, directed = False)\n",
    "        sense_feat_dict.append(\"PPR To Random Node \" + str(idx)) \n",
    "\n",
    "    print (\"Computing Hops to Core Random Nodes...\", end = '\\r')\n",
    "    for idx, node in tqdm(enumerate(core_anchors)):\n",
    "        sp_ = nx.single_source_shortest_path_length(graph, source = node)\n",
    "        sense_features[:, len(sense_feat_dict)] = [sp_[n] for n in range(len(graph))]\n",
    "        sense_feat_dict.append(\"Hops To Random Node \" + str(idx))\n",
    "\n",
    "\n",
    "    print (\"Normalizing Features Between 0 And 1...                   \", end = '\\r')\n",
    "    # Normalise to between 0 and 1 \n",
    "    sense_features = (sense_features - np.min(sense_features, axis = 0)) / np.ptp(sense_features, axis = 0)\n",
    "    sense_feat_dict = {sense_feat_dict[idx] : idx for idx in range(len(sense_feat_dict))}\n",
    "    \n",
    "    return sense_feat_dict, sense_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791fc317",
   "metadata": {},
   "source": [
    "### Sense Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4548a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_feature_membership(input_embed, embed_name, sense_features, sense_feat_dict, top_k = 8, gd_steps = 1000, solver = 'nmf', plot = False, constraints = False):\n",
    "    \n",
    "    \n",
    "    if solver == 'gd' :\n",
    "        # Tensorflow Variables For Optimization\n",
    "        # Input embedding - fixed\n",
    "        embeddings = tf.Variable(initial_value = input_embed,\n",
    "                    shape = input_embed.shape,\n",
    "                    dtype = tf.float32, trainable = False)\n",
    "\n",
    "        # Matrix explaining membership - trainable\n",
    "        explain = tf.Variable(initial_value = np.random.randn(input_embed.shape[1], sense_features.shape[1]),\n",
    "                        shape = (input_embed.shape[1], sense_features.shape[1]),\n",
    "                        dtype = tf.float32, trainable = True)\n",
    "\n",
    "        # Explainable features - fixed\n",
    "        sense = tf.Variable(initial_value = sense_features,\n",
    "                        shape = sense_features.shape,\n",
    "                        dtype = tf.float32, trainable = False)\n",
    "\n",
    "        # Set up an optimizer\n",
    "        optimizer = tf.keras.optimizers.Nadam(learning_rate = 0.001)\n",
    "\n",
    "        # Optimize \n",
    "        losses = []\n",
    "        for i in tqdm(range(gd_steps)):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                # Minimize || sense - embeddings @ explain ||\n",
    "                prod = tf.matmul(embeddings, explain)\n",
    "                loss = tf.norm(sense - prod, ord = 2)\n",
    "                \n",
    "                if constraints == True:\n",
    "                    loss = loss + (0.5 * tf.linalg.norm(tf.matmul(explain, explain, transpose_b = True))) + (0.5 * tf.math.reduce_sum(tf.linalg.norm(explain, axis = 0)))\n",
    "        \n",
    "            gradients = tape.gradient(loss, [explain])\n",
    "            optimizer.apply_gradients(zip(gradients, [explain]))\n",
    "            explain.assign(tf.clip_by_value(explain, clip_value_min = 0, clip_value_max = tf.math.reduce_max(explain)))\n",
    "            \n",
    "            losses.append(loss)\n",
    "        \n",
    "        reconstruction_loss = losses[-1]\n",
    "        print (\"Reconstruction Loss : \", float(loss))\n",
    "\n",
    "        # Default embed vector - assume one dimension only - non trainable\n",
    "        embeddings_default = tf.Variable(initial_value = np.ones((input_embed.shape[0], 1)),\n",
    "                                     shape = (input_embed.shape[0], 1),\n",
    "                                     dtype = tf.float32, trainable = False)\n",
    "\n",
    "        # Default explanantion vector - trainable\n",
    "        explain_default = tf.Variable(initial_value = np.random.randn(1, sense_features.shape[1]),\n",
    "                                      shape = (1, sense_features.shape[1]),\n",
    "                                      dtype = tf.float32, trainable = True)\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Nadam(learning_rate = 0.001)\n",
    "\n",
    "        default_losses = []\n",
    "        for i in tqdm(range(gd_steps)):\n",
    "            \n",
    "                                  \n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                prod = tf.matmul(embeddings_default, explain_default)\n",
    "                loss = tf.norm(sense - prod, ord = 2)\n",
    "\n",
    "            gradients = tape.gradient(loss, [explain_default])\n",
    "            optimizer.apply_gradients(zip(gradients, [explain_default]))\n",
    "            explain_default.assign(tf.clip_by_value(explain_default, clip_value_min = 0, clip_value_max = tf.math.reduce_max(explain_default)))\n",
    "            default_losses.append(loss)\n",
    "\n",
    "        print (\"Default Loss : \", float(loss))\n",
    "        \n",
    "    if solver == 'nmf':\n",
    "        \n",
    "        # Ensure proper dtypes\n",
    "        sense_features = sense_features.astype(np.float32)\n",
    "        input_embed = input_embed.astype(np.float32)\n",
    "\n",
    "        # Play around with transposes to make it make sense\n",
    "        explain, embed_recon, _ = non_negative_factorization(n_components = input_embed.shape[1],\n",
    "                                                                     init = 'custom',\n",
    "                                                                     max_iter = 4000,\n",
    "                                                                     X = sense_features.T,\n",
    "                                                                     H = input_embed.T,\n",
    "                                                                     update_H = False)\n",
    "\n",
    "        explain = explain.T\n",
    "        embed_recon = embed_recon.T\n",
    "\n",
    "        reconstruction_loss = np.linalg.norm(sense_features - (input_embed @ explain))\n",
    "        \n",
    "        default_embed = np.ones((input_embed.shape[0], 1)).astype(np.float32)\n",
    "        explain_default, _, _ = non_negative_factorization(n_components = default_embed.shape[1],\n",
    "                                                             init = 'custom',\n",
    "                                                             max_iter = 2000,\n",
    "                                                             X = sense_features.T,\n",
    "                                                             H = default_embed.T,\n",
    "                                                             update_H = False)\n",
    "        explain_default = explain_default.T\n",
    "        loss_2 = np.linalg.norm(sense_features - (default_embed @ explain_default))\n",
    "                            \n",
    "                                \n",
    "        \n",
    "    \n",
    "    # Normalize matrix by the default matrix learned\n",
    "    explain_norm = np.array(explain / explain_default)    \n",
    "    explain_norm_softmax = np.array([np.exp(x) / sum(np.exp(x)) for x in explain_norm])\n",
    "    explain_variance = np.square(np.std(explain_norm, axis = 1))\n",
    "    \n",
    "    # Plot variance in explanability of each dimension\n",
    "    embed_dimensions = input_embed.shape[1]\n",
    "\n",
    "    if plot: \n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Bar(x = list(range(embed_dimensions)), \n",
    "                             y = explain_variance,\n",
    "                             name = 'Variance of Embedding Dimensions'))\n",
    "        fig.add_trace(go.Scatter(x = list(range(embed_dimensions)), \n",
    "                                 y = [np.mean(explain_variance)] * embed_dimensions, \n",
    "                                 mode = 'lines', \n",
    "                                 name = 'Mean of Variance'))\n",
    "        fig.add_trace(go.Scatter(x = list(range(embed_dimensions)), \n",
    "                                 y = [np.median(explain_variance)] * embed_dimensions, \n",
    "                                 mode = 'lines', \n",
    "                                 name = 'Median of Variance'))\n",
    "        fig.update_layout(title_text = 'Variance of Explanability Across Dimensions - ' + embed_name,\n",
    "                          xaxis_title_text = 'Dimensions', \n",
    "                          yaxis_title_text = 'Variance')\n",
    "        fig.show()\n",
    "    \n",
    "    # Figure out which dimensions to keep - ones with most variance \n",
    "    dimensions_idx_to_keep = np.where(explain_variance > np.mean(explain_variance))[0]\n",
    "    dimensions_to_keep = np.array(explain_norm)[dimensions_idx_to_keep]\n",
    "    dimensions_to_keep_softmax = explain_norm_softmax[dimensions_idx_to_keep]\n",
    "    top_k_dims = np.argsort(explain_variance)[-top_k:]\n",
    "    \n",
    "    # Plot membership of sense features vs remaining dimensions\n",
    "    features = list(sense_feat_dict.keys())\n",
    "    \n",
    "    if plot: \n",
    "        fig = go.Figure()\n",
    "\n",
    "        for idx in range(len(dimensions_to_keep)):\n",
    "            fig.add_trace(go.Bar(x = features, \n",
    "                                 y = dimensions_to_keep[idx],\n",
    "                                 name = 'Dimension ' + str(dimensions_idx_to_keep[idx])))\n",
    "\n",
    "        fig.update_layout(title_text = 'Embedding Dimension Feature Membership - ' + embed_name,\n",
    "                          xaxis_title_text = 'Sense Features',\n",
    "                          yaxis_title_text = 'Membership',\n",
    "                          barmode = 'group')\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "    return_dict = {\n",
    "        'explain' : explain,\n",
    "        'explain_norm' : explain_norm,\n",
    "        'explain_default' : explain_default,\n",
    "        'dimensions_idx_to_keep' : dimensions_idx_to_keep,\n",
    "        'top_k_dims' : top_k_dims,\n",
    "        'reconstruction_loss' : reconstruction_loss\n",
    "    }\n",
    "    \n",
    "    return return_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f926bb3",
   "metadata": {},
   "source": [
    "### Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36777799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_model(input_shape):\n",
    "    \n",
    "    node_a = Input(shape = input_shape)\n",
    "    node_b = Input(shape = input_shape)\n",
    "    \n",
    "    X = Concatenate()([node_a, node_b])\n",
    "    X = Dense(64, activation = 'relu')(X)\n",
    "    X = Dense(2, activation = 'softmax')(X)\n",
    "    \n",
    "    return Model(inputs = [node_a, node_b], outputs = X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6277151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_perf(input_embed, input_dict, data = None, labels = None, graph = None, epochs = 200, hidden_edges = None, train_set = None, train_set_neg = None, test_set = None, test_set_neg = None):\n",
    "    \n",
    "    results = np.zeros((10, 1))\n",
    "            \n",
    "    # All Dimensions \n",
    "    all_train_acc, all_eval_acc, all_embed_dim, all_auc, all_aup  = get_link_perf(input_embed = input_embed,\n",
    "                                                               data = data, \n",
    "                                                               labels = labels,\n",
    "                                                               graph = graph,\n",
    "                                                               hidden_edges = hidden_edges, \n",
    "                                                               train_set = train_set, \n",
    "                                                               train_set_neg = train_set_neg, \n",
    "                                                               test_set = test_set, \n",
    "                                                               test_set_neg = test_set_neg, \n",
    "                                                                                 epochs = epochs)\n",
    "\n",
    "    # Important Dimensions \n",
    "    embed_imp = input_embed[:, input_dict['dimensions_idx_to_keep']]\n",
    "    imp_train_acc, imp_eval_acc, imp_embed_dim, imp_auc, imp_aup = get_link_perf(input_embed = embed_imp,\n",
    "                                                               data = data, \n",
    "                                                               labels = labels,\n",
    "                                                               graph = graph,\n",
    "                                                               hidden_edges = hidden_edges, \n",
    "                                                               train_set = train_set, \n",
    "                                                               train_set_neg = train_set_neg, \n",
    "                                                               test_set = test_set, \n",
    "                                                               test_set_neg = test_set_neg, \n",
    "                                                                                epochs = epochs)\n",
    "\n",
    "\n",
    "    results[:, 0] = all_train_acc, all_eval_acc, all_embed_dim, all_aup, all_auc, imp_train_acc, imp_eval_acc, imp_embed_dim, imp_aup, imp_auc#, top_train_acc, top_eval_acc, top_embed_dim, top_aup, top_auc\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "    results.index = ['Training Accuracy - All', 'Test Accuracy - All', 'Embedding Dimensions - All', 'AUP - All', 'AUC - All',\n",
    "                       'Training Accuracy - Thresholded', 'Test Accuracy - Thresholded', 'Embedding Dimensions - Thresholded', 'AUP - Thresholded', 'AUC - Thresholded',]\n",
    "\n",
    "    results.columns = ['Values']\n",
    "    # display (results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66ec4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_perf(input_embed, graph = None, hidden_edges = None, data = None, labels = None, train_set = None, train_set_neg = None, test_set = None, test_set_neg = None, epochs = 200, learning_rate = 0.001, train_size = 0.7, display_results = False, return_model = False, random_state = 2021):\n",
    "    \n",
    "    \n",
    "    embed_dim = input_embed.shape[1]\n",
    "    \n",
    "    if type(hidden_edges) == type(None):\n",
    "        X_0 = np.zeros((data.shape[0], embed_dim))\n",
    "        X_1 = np.zeros((data.shape[0], embed_dim))\n",
    "\n",
    "        for idx in tqdm(range(len(data))): \n",
    "\n",
    "            node_0 = data[idx][0]\n",
    "            node_1 = data[idx][1]\n",
    "\n",
    "            X_0[idx, :] = input_embed[node_0]\n",
    "            X_1[idx, :] = input_embed[node_1]\n",
    "\n",
    "        Y = to_categorical(labels)\n",
    "\n",
    "        X_0_train, X_0_test, X_1_train, X_1_test, y_train, y_test = train_test_split(X_0,\n",
    "                                                                                     X_1,\n",
    "                                                                                     Y,\n",
    "                                                                                     train_size = train_size,\n",
    "                                                                                     shuffle = True, \n",
    "                                                                                     random_state = random_state)\n",
    "    else: \n",
    "        \n",
    "        X_0_train, X_0_test, X_1_train, X_1_test, y_train, y_test = generate_link_data(input_embed = input_embed,\n",
    "                                                                                       train_set = train_set,\n",
    "                                                                                       train_set_neg = test_set_neg,\n",
    "                                                                                       test_set = test_set,\n",
    "                                                                                       test_set_neg = test_set_neg)\n",
    "    \n",
    "    model = decoder_model(input_shape = (embed_dim,))\n",
    "    model.compile(loss = tf.keras.losses.binary_crossentropy,\n",
    "                  optimizer = tf.keras.optimizers.Adam(learning_rate),\n",
    "                  metrics = [\"accuracy\"])\n",
    "    \n",
    "    history = model.fit([X_0_train, X_1_train], y_train, epochs = epochs)\n",
    "    eval_loss, eval_acc = model.evaluate([X_0_test, X_1_test], y_test)\n",
    "    \n",
    "    train_acc = history.history['accuracy'][-1]\n",
    "    \n",
    "    y_pred = model.predict([X_0_test, X_1_test])\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    aup = average_precision_score(y_test, y_pred)\n",
    "    \n",
    "    if return_model:\n",
    "        return train_acc, eval_acc, embed_dim, auc, aup, model\n",
    "    \n",
    "    return train_acc, eval_acc, embed_dim, auc, aup\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37661ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_link_data(input_embed, train_set, train_set_neg, test_set, test_set_neg):\n",
    "    \n",
    "    train_set = np.array(train_set)\n",
    "    train_set_neg = np.array(train_set_neg)\n",
    "    test_set = np.array(test_set)\n",
    "    test_set_neg = np.array(test_set_neg)\n",
    "    \n",
    "    train_data = np.vstack((train_set, train_set_neg))\n",
    "    train_labels = np.vstack((np.ones((train_set.shape[0], 1)), np.zeros((train_set_neg.shape[0], 1))))\n",
    "\n",
    "    test_data = np.vstack((np.array(test_set), test_set_neg))\n",
    "    test_labels = np.vstack((np.ones((len(test_set), 1)), np.zeros((test_set_neg.shape[0], 1))))\n",
    "    \n",
    "    # Put into right format \n",
    "    embed_dim = input_embed.shape[1]\n",
    "    X_0_train = np.zeros((train_data.shape[0], embed_dim))\n",
    "    X_1_train = np.zeros((train_data.shape[0], embed_dim))\n",
    "\n",
    "    X_0_test = np.zeros((test_data.shape[0], embed_dim))\n",
    "    X_1_test = np.zeros((test_data.shape[0], embed_dim))\n",
    "\n",
    "    for idx in tqdm(range(len(train_data))): \n",
    "\n",
    "        node_0 = train_data[idx][0]\n",
    "        node_1 = train_data[idx][1]\n",
    "\n",
    "        X_0_train[idx, :] = input_embed[node_0]\n",
    "        X_1_train[idx, :] = input_embed[node_1]\n",
    "\n",
    "    for idx in tqdm(range(len(test_data))): \n",
    "\n",
    "        node_0 = test_data[idx][0]\n",
    "        node_1 = test_data[idx][1]\n",
    "\n",
    "        X_0_test[idx, :] = input_embed[node_0]\n",
    "        X_1_test[idx, :] = input_embed[node_1]\n",
    "\n",
    "    Y_train = to_categorical(train_labels)\n",
    "    Y_test = to_categorical(test_labels)\n",
    "\n",
    "    print (\"Train Data : \", train_data.shape)\n",
    "    print (\"Test Data : \", test_data.shape) \n",
    "\n",
    "    print (\"X0 Train: \", X_0_train.shape)\n",
    "    print (\"X1 Train: \", X_1_train.shape)\n",
    "    print (\"X0 Test: \", X_0_test.shape)\n",
    "    print (\"X1 Test: \", X_1_test.shape)\n",
    "    print (\"Y Train: \", Y_train.shape)\n",
    "    print (\"Y Test: \", Y_test.shape)\n",
    "    \n",
    "    return X_0_train, X_0_test, X_1_train, X_1_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c2cbf7",
   "metadata": {},
   "source": [
    "### Miscellanious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ea1d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(mat_1, mat_2):\n",
    "    \n",
    "    # Initialise empty distance matrix\n",
    "    distances = np.zeros((mat_1.shape[0], mat_2.shape[0]))\n",
    "    \n",
    "    # Iterate through all data points of Mat_1\n",
    "    for idx_one in tqdm(range(len(mat_1))):\n",
    "\n",
    "        # Iterate through all data points of Mat_2\n",
    "        for idx_two in range(len(mat_2)):\n",
    "\n",
    "            # Set the symmetric distances\n",
    "            distances[idx_one][idx_two] = np.linalg.norm(mat_1[idx_one] - mat_2[idx_two])\n",
    "            \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d295c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion(y_true, y_pred, normalize = 'true'):\n",
    "    conf = confusion_matrix(y_true, y_pred, normalize = normalize)\n",
    "    conf = pd.DataFrame(conf)\n",
    "    conf.columns = ['Predicted False', 'Predicted True']\n",
    "    conf.index = ['Label False', 'Label True']\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f841f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(in_dict, reverse = False):\n",
    "\n",
    "    '''\n",
    "        Returns a dictionary sorted by keys \n",
    "        Inputs : \n",
    "            in_dict : dict - Dictionary to sort\n",
    "            reverse : Bool - Ascending / Descending\n",
    "    '''\n",
    "\n",
    "    return sorted(in_dict.items(), key = lambda kv : kv[1], reverse = reverse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff4ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
