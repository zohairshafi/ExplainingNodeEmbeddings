{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b8e68c4",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a7ee943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm\n",
    "from DGI.models import DGI, LogReg\n",
    "from DGI.utils import process\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "\n",
    "from scripts.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c7f22",
   "metadata": {},
   "source": [
    "### Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60e95d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbedder:\n",
    "    def __init__(self, graph, embed_shape = (128,)):\n",
    "        self.embed(graph)\n",
    "        self.E = list(graph.edges())\n",
    "        self.graph = graph\n",
    "        self.embed_shape = embed_shape\n",
    "    \n",
    "    def embed(self, graph):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        raise NotImplementedError\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "21ecb4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGIEmbedding(BaseEmbedder):\n",
    "    def __init__(self, embed_dim = 64, graph = None, feature_matrix = None, use_xm = False, debug = False, batch_size = 1, nb_epochs = 2500, patience = 20, ortho_ = 0.1, sparse_ = 0.1, lr = 1e-3, l2_coef = 0.0, drop_prob = 0.0, sparse = True, nonlinearity = 'prelu'):\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Training Params\n",
    "        self.graph = graph\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.patience = patience\n",
    "        self.lr = lr\n",
    "        self.l2_coef = l2_coef\n",
    "        self.feature_matrix = feature_matrix\n",
    "        self.drop_prob = drop_prob\n",
    "        self.hid_units = embed_dim\n",
    "        self.sparse = sparse\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.use_xm = use_xm\n",
    "        self.ortho_ = ortho_\n",
    "        self.sparse_ = sparse_\n",
    "        \n",
    "        if graph is not None:\n",
    "            self.embed()\n",
    "        else:\n",
    "            self.graph = None\n",
    "    \n",
    "    def embed(self):\n",
    "\n",
    "        \n",
    "        if self.feature_matrix is None:\n",
    "            feature_matrix = np.identity(len(self.graph))\n",
    "        else: \n",
    "            feature_matrix = self.feature_matrix\n",
    "\n",
    "        adj = nx.to_scipy_sparse_array(self.graph)\n",
    "        features = sp.lil_matrix(feature_matrix)\n",
    "        features, _ = process.preprocess_features(features)\n",
    "\n",
    "        nb_nodes = features.shape[0]\n",
    "        ft_size = features.shape[1]\n",
    "\n",
    "        adj = process.normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "        if self.sparse:\n",
    "            sp_adj = process.sparse_mx_to_torch_sparse_tensor(adj)\n",
    "        else:\n",
    "            adj = (adj + sp.eye(adj.shape[0])).todense()\n",
    "\n",
    "        features = torch.FloatTensor(features[np.newaxis])\n",
    "        if not self.sparse:\n",
    "            adj = torch.FloatTensor(adj[np.newaxis])\n",
    "\n",
    "        if self.feature_matrix is not None: \n",
    "            sense_features = torch.FloatTensor(self.feature_matrix)\n",
    "\n",
    "\n",
    "        model = DGI(ft_size, self.hid_units, self.nonlinearity)\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr = self.lr, weight_decay = self.l2_coef)\n",
    "\n",
    "        b_xent = nn.BCEWithLogitsLoss()\n",
    "        xent = nn.CrossEntropyLoss()\n",
    "        cnt_wait = 0\n",
    "        best = 1e9\n",
    "        best_t = 0\n",
    "\n",
    "        for epoch in tqdm(range(self.nb_epochs)):\n",
    "            model.train()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            idx = np.random.permutation(nb_nodes)\n",
    "            shuf_fts = features[:, idx, :]\n",
    "\n",
    "            lbl_1 = torch.ones(self.batch_size, nb_nodes)\n",
    "            lbl_2 = torch.zeros(self.batch_size, nb_nodes)\n",
    "            lbl = torch.cat((lbl_1, lbl_2), 1)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                shuf_fts = shuf_fts.cuda()\n",
    "                lbl = lbl.cuda()\n",
    "\n",
    "            logits = model(features, shuf_fts, sp_adj if self.sparse else adj, self.sparse, None, None, None) \n",
    "            \n",
    "            if self.use_xm == True and feature_matrix is not None:\n",
    "                \n",
    "                sf = sense_features\n",
    "                embeds, _ = model.embed(sf, sp_adj if self.sparse else adj, self.sparse, None)\n",
    "                embeds = torch.squeeze(embeds)\n",
    "                sense_mat = torch.einsum('ij, ik -> ijk', embeds, sf)\n",
    "                E = sense_mat\n",
    "                y_norm = torch.diagonal(torch.matmul(logits, torch.transpose(logits, 0, 1)))\n",
    "                sense_norm = torch.diagonal(torch.matmul(sf, torch.transpose(sf, 0, 1)))\n",
    "                norm = torch.multiply(y_norm, sense_norm)\n",
    "                E = torch.transpose(torch.transpose(E, 0, 2) / norm, 0, 2)\n",
    "                \n",
    "                E_t = torch.transpose(E, 1, 2)\n",
    "                E_o = torch.einsum('aij, ajh -> aih', E, E_t)\n",
    "                E_o = torch.sum(E_o)\n",
    "                ortho_loss = (self.ortho_ * E_o) / self.batch_size\n",
    "                \n",
    "                sparse_loss = (self.sparse_ * torch.sum(torch.linalg.norm(E, ord = 1, axis = 0))) / self.batch_size\n",
    "\n",
    "            loss = b_xent(logits, lbl) + ortho_loss + sparse_loss\n",
    "\n",
    "            if self.debug:\n",
    "                print('Loss:', loss)\n",
    "\n",
    "            if loss < best:\n",
    "                best = loss\n",
    "                best_t = epoch\n",
    "                cnt_wait = 0\n",
    "                torch.save(model.state_dict(), 'best_dgi.pkl')\n",
    "            else:\n",
    "                cnt_wait += 1\n",
    "\n",
    "            if cnt_wait == self.patience:\n",
    "                if self.debug: \n",
    "                    print('Early stopping!')\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "        if self.debug: \n",
    "            print('Loading {}th epoch'.format(best_t))\n",
    "        model.load_state_dict(torch.load('best_dgi.pkl'))\n",
    "\n",
    "        self.node_model = model\n",
    "        self.fitted = True\n",
    "\n",
    "        embeds, _ = model.embed(features, sp_adj if self.sparse else adj, self.sparse, None)\n",
    "        self.embeddings = embeds\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        return np.squeeze(self.embeddings.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e19b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd074ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee2e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082cce1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
