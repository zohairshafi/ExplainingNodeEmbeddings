{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0369cae7",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "707c0344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zohairshafi/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm\n",
    "from DGI.models import DGI, LogReg\n",
    "from DGI.utils import process\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "\n",
    "from scripts.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba79c95f",
   "metadata": {},
   "source": [
    "### Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9319bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbedder:\n",
    "    def __init__(self, graph, embed_shape = (128,)):\n",
    "        self.embed(graph)\n",
    "        self.E = list(graph.edges())\n",
    "        self.graph = graph\n",
    "        self.embed_shape = embed_shape\n",
    "    \n",
    "    def embed(self, graph):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        raise NotImplementedError\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41610868",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGIEmbedding(BaseEmbedder):\n",
    "    def __init__(self, embed_dim = 64, graph = None, feature_matrix = None, use_xm = False, debug = False, batch_size = 1, nb_epochs = 2500, patience = 20, ortho_ = 0.1, sparse_ = 0.1, lr = 1e-3, l2_coef = 0.0, drop_prob = 0.0, sparse = True, nonlinearity = 'prelu', model_name = ''):\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Training Params\n",
    "        self.graph = graph\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.patience = patience\n",
    "        self.lr = lr\n",
    "        self.l2_coef = l2_coef\n",
    "        self.feature_matrix = feature_matrix\n",
    "        self.drop_prob = drop_prob\n",
    "        self.hid_units = embed_dim\n",
    "        self.sparse = sparse\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.use_xm = use_xm\n",
    "        self.ortho_ = ortho_\n",
    "        self.sparse_ = sparse_\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.time_per_epoch = None\n",
    "        \n",
    "        if graph is not None:\n",
    "            self.embed()\n",
    "        else:\n",
    "            self.graph = None\n",
    "    \n",
    "    def embed(self):\n",
    "\n",
    "        \n",
    "        if self.feature_matrix is None:\n",
    "            feature_matrix = np.identity(len(self.graph))\n",
    "        else: \n",
    "            feature_matrix = self.feature_matrix\n",
    "\n",
    "        adj = nx.to_scipy_sparse_array(self.graph)\n",
    "        features = sp.lil_matrix(feature_matrix)\n",
    "        features, _ = process.preprocess_features(features)\n",
    "\n",
    "        nb_nodes = features.shape[0]\n",
    "        ft_size = features.shape[1]\n",
    "\n",
    "        adj = process.normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "        if self.sparse:\n",
    "            sp_adj = process.sparse_mx_to_torch_sparse_tensor(adj)\n",
    "        else:\n",
    "            adj = (adj + sp.eye(adj.shape[0])).todense()\n",
    "\n",
    "        features = torch.FloatTensor(features[np.newaxis])\n",
    "        if not self.sparse:\n",
    "            adj = torch.FloatTensor(adj[np.newaxis])\n",
    "            if torch.cuda.is_available():\n",
    "                adj = adj.cuda()\n",
    "\n",
    "        if self.feature_matrix is not None: \n",
    "            sense_features = torch.FloatTensor(self.feature_matrix)\n",
    "            if torch.cuda.is_available():\n",
    "                sense_features = sense_features.cuda()\n",
    "\n",
    "\n",
    "        model = DGI(ft_size, self.hid_units, self.nonlinearity)\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr = self.lr, weight_decay = self.l2_coef)\n",
    "\n",
    "        b_xent = nn.BCEWithLogitsLoss()\n",
    "        xent = nn.CrossEntropyLoss()\n",
    "        cnt_wait = 0\n",
    "        best = 1e9\n",
    "        best_t = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for epoch in tqdm(range(self.nb_epochs)):\n",
    "            model.train()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            idx = np.random.permutation(nb_nodes)\n",
    "            shuf_fts = features[:, idx, :]\n",
    "\n",
    "            lbl_1 = torch.ones(self.batch_size, nb_nodes)\n",
    "            lbl_2 = torch.zeros(self.batch_size, nb_nodes)\n",
    "            lbl = torch.cat((lbl_1, lbl_2), 1)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                shuf_fts = shuf_fts.cuda()\n",
    "                lbl = lbl.cuda()\n",
    "                sp_adj = sp_adj.cuda()\n",
    "                features = features.cuda()\n",
    "            \n",
    "            logits = model(features, shuf_fts, sp_adj if self.sparse else adj, self.sparse, None, None, None) \n",
    "            \n",
    "            if self.use_xm == True and feature_matrix is not None:\n",
    "                \n",
    "                start_idx = 0\n",
    "                loop = True\n",
    "                \n",
    "                ortho_loss = 0\n",
    "                sparse_loss = 0\n",
    "                xm_batch_size = 1024\n",
    "                \n",
    "                sf = sense_features\n",
    "                embeds, _ = model.embed(sf, sp_adj if self.sparse else adj, self.sparse, None)\n",
    "                                \n",
    "                while loop:\n",
    "                    end_idx = start_idx + xm_batch_size\n",
    "                    if end_idx > len(self.graph):\n",
    "                        loop = False\n",
    "                        end_idx = len(self.graph)\n",
    "                        \n",
    "                    \n",
    "                    sf = sense_features[start_idx : end_idx]\n",
    "                    embeds_ = torch.squeeze(embeds)[start_idx : end_idx]\n",
    "                    \n",
    "                    \n",
    "                    sense_mat = torch.einsum('ij, ik -> ijk', embeds_, sf)\n",
    "                    E = sense_mat\n",
    "                    y_norm = torch.diagonal(torch.matmul(embeds_, torch.transpose(embeds_, 0, 1)))\n",
    "                    sense_norm = torch.diagonal(torch.matmul(sf, torch.transpose(sf, 0, 1)))\n",
    "                    norm = torch.multiply(y_norm, sense_norm)\n",
    "                    E = torch.transpose(torch.transpose(E, 0, 2) / norm, 0, 2)\n",
    "                    E = (E - torch.amin(E, dim = [-1, -2], keepdim = True)) / (torch.amax(E, dim = [-1, -2], keepdim = True) - torch.amin(E, dim = [-1, -2], keepdim = True))\n",
    "\n",
    "                    E_t = torch.transpose(E, 1, 2)\n",
    "                    E_o = torch.einsum('aij, ajh -> aih', E, E_t)\n",
    "                    E_o = torch.sum(E_o)\n",
    "                    batch_ortho_loss = (self.ortho_ * E_o) / self.batch_size\n",
    "\n",
    "                    batch_sparse_loss = (self.sparse_ * torch.sum(torch.linalg.norm(E, ord = 1, axis = 0))) / self.batch_size\n",
    "                        \n",
    "                    ortho_loss += batch_ortho_loss\n",
    "                    sparse_loss += batch_sparse_loss\n",
    "                    \n",
    "                    start_idx = end_idx\n",
    "                    \n",
    "                loss = b_xent(logits, lbl) + ortho_loss + sparse_loss\n",
    "            else:\n",
    "                loss = b_xent(logits, lbl)\n",
    "\n",
    "            if self.debug:\n",
    "                print('Loss:', loss)\n",
    "\n",
    "            if loss < best:\n",
    "                best = loss\n",
    "                best_t = epoch\n",
    "                cnt_wait = 0\n",
    "                torch.save(model.state_dict(), self.model_name + '.pkl')\n",
    "            else:\n",
    "                cnt_wait += 1\n",
    "\n",
    "            if cnt_wait == self.patience:\n",
    "                if self.debug: \n",
    "                    print('Early stopping!')\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "        self.time_per_epoch = (time.time() - start_time) / epoch\n",
    "\n",
    "        if self.debug: \n",
    "            print('Loading {}th epoch'.format(best_t))\n",
    "        model.load_state_dict(torch.load(self.model_name + '.pkl'))\n",
    "\n",
    "        self.node_model = model\n",
    "        self.fitted = True\n",
    "\n",
    "        embeds, _ = model.embed(features, sp_adj if self.sparse else adj, self.sparse, None)\n",
    "        self.embeddings = embeds\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        if torch.cuda.is_available():\n",
    "            return np.squeeze(self.embeddings.cpu().numpy())\n",
    "        return np.squeeze(self.embeddings.numpy())\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
