{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe3d6f9",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c556d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import numpy.random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import linalg as spla\n",
    "\n",
    "from tensorflow.keras import layers, models, Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, Dropout, Concatenate, LSTM, Bidirectional, Add, Subtract, Lambda\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, History\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from collections import deque\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fdb0f5",
   "metadata": {},
   "source": [
    "### SDNE+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23cdfbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nxgraph(graph):\n",
    "    node2idx = {}\n",
    "    idx2node = []\n",
    "    node_size = 0\n",
    "    for node in graph.nodes():\n",
    "        node2idx[node] = node_size\n",
    "        idx2node.append(node)\n",
    "        node_size += 1\n",
    "    return idx2node, node2idx\n",
    "\n",
    "def l_2nd(beta):\n",
    "    def loss_2nd(y_true, y_pred):\n",
    "        \n",
    "        b_ = (tf.cast((y_true > 0), tf.float32) * beta)\n",
    "        x = K.square((y_true - y_pred) * b_)\n",
    "        t = K.sum(x, axis = -1, )\n",
    "        return K.mean(t)\n",
    "\n",
    "    return loss_2nd\n",
    "\n",
    "\n",
    "def l_1st_plus(alpha):\n",
    "    def loss_1st(y_true, y_pred):\n",
    "        \n",
    "        L = y_true\n",
    "        Y = y_pred\n",
    "\n",
    "        batch_size = tf.cast(K.shape(L)[0], np.float32)\n",
    "        l_1 = alpha * 2 * tf.linalg.trace(tf.matmul(tf.matmul(Y, L, transpose_a = True), Y)) / batch_size\n",
    "        \n",
    "        return l_1\n",
    "\n",
    "    return loss_1st\n",
    "\n",
    "def l_ortho(gamma, embed_dim):\n",
    "    \n",
    "    def loss_3rd(y_true, y_pred):\n",
    "        \n",
    "        E = y_pred\n",
    "        A = y_true\n",
    "        \n",
    "        batch_size = tf.cast(K.shape(A)[0], np.float32)\n",
    "        \n",
    "        return gamma * E / batch_size\n",
    "    \n",
    "    return loss_3rd\n",
    "    \n",
    "\n",
    "def l_sparse(delta):\n",
    "    \n",
    "    def loss_4th(y_true, y_pred):\n",
    "        \n",
    "        E = y_pred\n",
    "        sense = y_true     \n",
    "        batch_size = tf.cast(K.shape(E)[0], np.float32)\n",
    "        \n",
    "        return delta * tf.reduce_sum(tf.norm(E, ord = 1, axis = 0)) / batch_size\n",
    "    \n",
    "    return loss_4th\n",
    "    \n",
    "\n",
    "def create_model_plus(node_size, sense_feat_size, hidden_size = [256, 128], l1 = 1e-5, l2 = 1e-4):\n",
    "    \n",
    "    A = Input(shape = (node_size,))\n",
    "    A_2 = Input(shape = (None,))\n",
    "    L = Input(shape = (None,))\n",
    "    sense = Input(shape = (sense_feat_size, ))\n",
    "    \n",
    "    \n",
    "    fc = A\n",
    "    for i in range(len(hidden_size)):\n",
    "        if i == len(hidden_size) - 1:\n",
    "            fc = Dense(hidden_size[i], activation = 'relu',\n",
    "                       kernel_regularizer = l1_l2(l1, l2), name = '1st')(fc)\n",
    "        else:\n",
    "            fc = Dense(hidden_size[i], activation = 'relu',\n",
    "                       kernel_regularizer = l1_l2(l1, l2))(fc)\n",
    "            \n",
    "    fc = tf.clip_by_value(fc, clip_value_min = 1e-10, clip_value_max = tf.math.reduce_max(fc), name = '1st')\n",
    "    Y = fc\n",
    "    \n",
    "    ####\n",
    "    sense_mat = tf.einsum('ij, ik -> ijk', Y, sense)\n",
    "    E = sense_mat\n",
    "    y_norm = tf.linalg.diag_part(tf.matmul(Y, Y, transpose_b = True), k = 0)\n",
    "    sense_norm = tf.linalg.diag_part(tf.matmul(sense, sense, transpose_b = True), k = 0)\n",
    "    norm = tf.multiply(y_norm, sense_norm)\n",
    "    E = tf.transpose(tf.transpose(E) / norm)\n",
    "    \n",
    "    \n",
    "    E_t = tf.transpose(E, perm = [0, 2, 1]) \n",
    "    E_1 = tf.einsum('aij, ajh -> aih', E, E_t)\n",
    "    E_1 = tf.reduce_sum(E_1)\n",
    "    \n",
    "    \n",
    "    E_2 = tf.multiply(1.0, E, name = 'sparse_loss')\n",
    "    ####\n",
    "    \n",
    "    \n",
    "    for i in reversed(range(len(hidden_size) - 1)):\n",
    "        fc = Dense(hidden_size[i], activation = 'relu',\n",
    "                   kernel_regularizer = l1_l2(l1, l2))(fc)\n",
    "\n",
    "    A_ = Dense(node_size, 'relu', name = '2nd')(fc)\n",
    "        \n",
    "    model = Model(inputs = [A, L, A_2, sense], outputs = [A_, Y, E_1, E_2])\n",
    "    emb = Model(inputs = A, outputs = Y)\n",
    "    return model, emb\n",
    "\n",
    "class SDNE_plus(object):\n",
    "    def __init__(self, graph, sense_features, lr = 1e-5, hidden_size = [32, 16], alpha = 1e-6, beta = 5., gamma = 0.1, delta = 0.1, nu1 = 1e-5, nu2 = 1e-4):\n",
    "\n",
    "        self.graph = graph\n",
    "        self.idx2node, self.node2idx = preprocess_nxgraph(self.graph)\n",
    "\n",
    "        self.node_size = self.graph.number_of_nodes()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "        self.nu1 = nu1\n",
    "        self.nu2 = nu2\n",
    "        self.sense_features = sense_features\n",
    "        self.lr = lr\n",
    "        \n",
    "\n",
    "        self.A, self.L = self._create_A_L(\n",
    "            self.graph, self.node2idx)  # Adj Matrix, L Matrix\n",
    "        self.reset_model()\n",
    "        self.inputs = [self.A, self.L]\n",
    "        self._embeddings = {}\n",
    "        \n",
    "\n",
    "    def reset_model(self, opt = 'adam'):\n",
    "\n",
    "        self.model, self.emb_model = create_model_plus(self.node_size,\n",
    "                                                      hidden_size = self.hidden_size,\n",
    "                                                      sense_feat_size = self.sense_features.shape[1],\n",
    "                                                      l1 = self.nu1,\n",
    "                                                      l2 = self.nu2)\n",
    "\n",
    "        opt = Nadam(learning_rate = self.lr)\n",
    "\n",
    "        self.model.compile(opt,\n",
    "                           [l_2nd(self.beta),\n",
    "                            l_1st_plus(self.alpha),\n",
    "                            l_ortho(self.gamma, self.hidden_size[-1]),\n",
    "                            l_sparse(self.delta),\n",
    "                           ])\n",
    "        self.get_embeddings()\n",
    "\n",
    "    def train(self, batch_size = 1, epochs = 1, initial_epoch = 0, verbose = 1):\n",
    "                \n",
    "        if batch_size >= self.node_size:\n",
    "            if batch_size > self.node_size:\n",
    "                print('batch_size({0}) > node_size({1}),set batch_size = {1}'.format(\n",
    "                    batch_size, self.node_size))\n",
    "                batch_size = self.node_size\n",
    "            return self.model.fit([self.A.todense(), self.L.todense(), self.A.todense(), self.sense_features],\n",
    "                                  [self.A.todense(), self.L.todense(), self.A.todense(), self.sense_features],\n",
    "                                  batch_size = batch_size, epochs = epochs, initial_epoch = initial_epoch, verbose = verbose,\n",
    "                                  shuffle=False, )\n",
    "        else:\n",
    "            steps_per_epoch = (self.node_size - 1) // batch_size + 1\n",
    "            hist = History()\n",
    "            hist.on_train_begin()\n",
    "            logs = {}\n",
    "            for epoch in range(initial_epoch, epochs):\n",
    "                start_time = time.time()\n",
    "                losses = np.zeros(5)\n",
    "                for i in range(steps_per_epoch):\n",
    "                    index = np.arange(\n",
    "                        i * batch_size, min((i + 1) * batch_size, self.node_size))\n",
    "                    A_train = self.A[index, :].todense()\n",
    "                    A_sub = self.A[index, :]\n",
    "                    A_sub = A_sub[:, index].todense()\n",
    "                    L_mat_train = self.L[index][:, index].todense()\n",
    "                                        \n",
    "                    inp = [A_train, L_mat_train, A_sub, self.sense_features[index, :]]\n",
    "                    oup = [A_train, L_mat_train, A_sub, self.sense_features[index, :]]\n",
    "                    \n",
    "                    batch_losses = self.model.train_on_batch(inp, oup)\n",
    "                    losses += batch_losses\n",
    "                losses = losses / steps_per_epoch\n",
    "\n",
    "                logs['loss'] = losses[0]\n",
    "                logs['2nd_loss'] = losses[1]\n",
    "                logs['1st_loss'] = losses[2]\n",
    "                logs['sparse_loss'] = losses[3]\n",
    "                logs['ortho_loss'] = losses[4]\n",
    "                epoch_time = int(time.time() - start_time)\n",
    "                #hist.on_epoch_end(epoch, logs)\n",
    "                if verbose > 0:\n",
    "                    print('Epoch {0}/{1}'.format(epoch + 1, epochs))\n",
    "                    print('{0}s - loss: {1: .4f} - 2nd_loss: {2: .4f} - 1st_loss: {3: .4f} - ortho_loss : {4: .4f} - sparse_loss : {5: .4f}'.format(\n",
    "                        epoch_time, losses[0], losses[1], losses[2], losses[3], losses[4]))\n",
    "            return hist\n",
    "\n",
    "    def evaluate(self, ):\n",
    "        return self.model.evaluate(x = self.inputs, y = self.inputs, batch_size = self.node_size)\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        self._embeddings = {}\n",
    "        \n",
    "        dense = self.A\n",
    "        \n",
    "        batch_size = dense.shape[0] // 10\n",
    "        \n",
    "        embeddings_1 = self.emb_model.predict(dense[:1 * batch_size].todense(), batch_size = batch_size)\n",
    "        embed_list = []\n",
    "        embed_list.append(embeddings_1)\n",
    "        for idx in range(1, 9):\n",
    "            embed_list.append(self.emb_model.predict(dense[idx * batch_size:(idx + 1) * batch_size].todense(), batch_size = batch_size))\n",
    "        embeddings_n = embed_list.append(self.emb_model.predict(dense[9 * batch_size:].todense(), batch_size = batch_size))\n",
    "        embeddings = np.vstack(embed_list)\n",
    "\n",
    "        assert embeddings.shape[0] == dense.shape[0]\n",
    "\n",
    "        look_back = self.idx2node\n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            self._embeddings[look_back[i]] = embedding\n",
    "\n",
    "        return self._embeddings\n",
    "\n",
    "    def _create_A_L(self, graph, node2idx):\n",
    "        node_size = graph.number_of_nodes()\n",
    "        A_data = []\n",
    "        A_row_index = []\n",
    "        A_col_index = []\n",
    "\n",
    "        for edge in graph.edges():\n",
    "            v1, v2 = edge\n",
    "            edge_weight = graph[v1][v2].get('weight', 1)\n",
    "\n",
    "            A_data.append(edge_weight)\n",
    "            A_row_index.append(node2idx[v1])\n",
    "            A_col_index.append(node2idx[v2])\n",
    "\n",
    "        A = sp.csr_matrix((A_data, (A_row_index, A_col_index)), shape = (node_size, node_size))\n",
    "        A_ = sp.csr_matrix((A_data + A_data, (A_row_index + A_col_index, A_col_index + A_row_index)),\n",
    "                           shape=(node_size, node_size))\n",
    "\n",
    "        D = sp.diags(A_.sum(axis=1).flatten().tolist()[0])\n",
    "        L = D - A_\n",
    "        return A, L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8252f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
