{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e23f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d034452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4998501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './email.pkl'\n",
    "run_count = 1\n",
    "hyp_key = 'hyp_email'\n",
    "outfile = './email_test.pkl'\n",
    "\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-g\", \"--graph_path\", required = True, help = 'Path to an nx.Graph object stored as a .pkl file')\n",
    "# ap.add_argument(\"-r\", \"--run_count\", required = True, help = \"Number of iterations for the experiment\", default = 1)\n",
    "# ap.add_argument(\"-k\", \"--hyp_key\", required = True, help = \"Key to index the hyperparameter json file\")\n",
    "# ap.add_argument(\"-o\", \"--outfile\", required = True, help = \"File name to save results into\")\n",
    "\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "# filename = args['graph_path']\n",
    "# run_count = args['run_count']\n",
    "# hyp_key = args['hyp_key']\n",
    "# outfile = args['outfile']\n",
    "\n",
    "#################################\n",
    "######### Read In Graph #########\n",
    "#################################\n",
    "with open(filename, 'rb') as file: \n",
    "    graph_dict = pkl.load(file)\n",
    "    \n",
    "graph = nx.Graph(nx.to_numpy_array(graph_dict['graph']))    \n",
    "graph = nx.Graph(nx.to_numpy_array(graph))\n",
    "\n",
    "\n",
    "#################################\n",
    "#### Generate Sense Features ####\n",
    "#################################\n",
    "sense_feat_dict, sense_features = get_sense_features(graph, ppr_flag = 'std')\n",
    "\n",
    "uncorrelated_feats = ['Degree',\n",
    "                    'Clustering Coefficient',\n",
    "                    'Personalized Page Rank - Standard Deviation',\n",
    "                    'Average Neighbor Degree',\n",
    "                    'Average Neighbor Clustering',\n",
    "                    'Eccentricity',\n",
    "                    'Katz Centrality']\n",
    "sense_features = sense_features[:, [list(sense_feat_dict).index(feat) for feat in uncorrelated_feats]]\n",
    "sense_feat_dict = {feat : idx for idx, feat in enumerate(uncorrelated_feats)}\n",
    "\n",
    "#################################\n",
    "######## Hyperparameters ########\n",
    "#################################\n",
    "\n",
    "# Define static ones to override or read in from a file\n",
    "\n",
    "if hyp_key == '':\n",
    "    hyp = {'sdne' : {'alpha' : 0.1, \n",
    "                     'beta' : 10, \n",
    "                     'gamma' : 0, \n",
    "                     'delta' : 0, \n",
    "                     'epochs' : 200, \n",
    "                     'batch_size' : 1024, \n",
    "                     'lr' : 1e-3}, \n",
    "\n",
    "          'sdne+xm' : {'alpha' : 1, \n",
    "                      'beta' : 1, \n",
    "                      'gamma' : 10, \n",
    "                      'delta' : 10, \n",
    "                      'epochs' : 400, \n",
    "                      'batch_size' : 1024, \n",
    "                      'lr' : 5e-4}}\n",
    "else: \n",
    "    with open('scripts/hyp.json', 'r') as file: \n",
    "        hyp_file = json.load(file)\n",
    "        hyp = hyp_file[hyp_key]\n",
    "\n",
    "\n",
    "#################################\n",
    "######## Run Experiment #########\n",
    "#################################\n",
    "\n",
    "dimensions = [16, 32, 64, 256, 512]\n",
    "results = {d : {} for d in dimensions}\n",
    "\n",
    "for run_idx in tqdm(range(run_count)):\n",
    "    \n",
    "    for d in dimensions: \n",
    "    \n",
    "        # Embed \n",
    "        \n",
    "        # Standard SDNE\n",
    "        sdne = SDNE_plus(graph, \n",
    "                          hidden_size = [32, d], \n",
    "                          lr = hyp['sdne']['lr'],\n",
    "                          sense_features = sense_features.astype(np.float32),\n",
    "                          alpha = hyp['sdne']['alpha'], \n",
    "                          beta = hyp['sdne']['beta'], \n",
    "                          gamma = hyp['sdne']['gamma'], \n",
    "                          delta = hyp['sdne']['delta'])\n",
    "        history = sdne.train(epochs = hyp['sdne']['epochs'], batch_size = hyp['sdne']['batch_size'])\n",
    "        e = sdne.get_embeddings()\n",
    "        embed_og = np.array([e[node_name] for node_name in graph.nodes()])\n",
    "        embed_og = (embed_og - np.min(embed_og)) / np.ptp(embed_og)\n",
    "\n",
    "        # SDNE+XM\n",
    "        sdne_plus = SDNE_plus(graph, \n",
    "                                  hidden_size = [32, d], \n",
    "                                  lr = hyp['sdne+xm']['lr'],\n",
    "                                  sense_features = sense_features.astype(np.float32),\n",
    "                                  alpha = hyp['sdne+xm']['alpha'], \n",
    "                                  beta = hyp['sdne+xm']['beta'], \n",
    "                                  gamma = hyp['sdne+xm']['gamma'], \n",
    "                                  delta = hyp['sdne+xm']['delta'])\n",
    "\n",
    "        sdne_plus.model.set_weights(sdne.model.get_weights())\n",
    "        history = sdne_plus.train(epochs = hyp['sdne+xm']['epochs'], batch_size = hyp['sdne+xm']['batch_size'])\n",
    "        e = sdne_plus.get_embeddings()\n",
    "        embed_plus = np.array([e[node_name] for node_name in graph.nodes()])\n",
    "        embed_plus = (embed_plus - np.min(embed_plus)) / np.ptp(embed_plus)\n",
    "        \n",
    "        # Generate Graph Explanations and Save\n",
    "        feature_dict_og = find_feature_membership(input_embed = embed_og,\n",
    "                                                    embed_name = 'SDNE',\n",
    "                                                    sense_features = sense_features,\n",
    "                                                    sense_feat_dict = sense_feat_dict,\n",
    "                                                    top_k = 8,\n",
    "                                                    solver = 'nmf')\n",
    "\n",
    "        explain_og = feature_dict_og['explain_norm']\n",
    "        explain_og = (explain_og - np.min(explain_og)) / np.ptp(explain_og)\n",
    "        explain_og_norm = np.linalg.norm(explain_og, ord = 'nuc')\n",
    "        \n",
    "        feature_dict_plus = find_feature_membership(input_embed = embed_plus,\n",
    "                                                            embed_name = 'SDNE+ Init',\n",
    "                                                            sense_features = sense_features,\n",
    "                                                            sense_feat_dict = sense_feat_dict,\n",
    "                                                            top_k = 8,\n",
    "                                                            solver = 'nmf')\n",
    "\n",
    "        explain_plus = feature_dict_plus['explain_norm']\n",
    "        explain_plus = (explain_plus - np.min(explain_plus)) / np.ptp(explain_plus)\n",
    "        explain_plus_norm = np.linalg.norm(explain_plus, ord = 'nuc')\n",
    "\n",
    "        # Generate Node Explanations\n",
    "        Y_og = embed_og\n",
    "        sense_mat = tf.einsum('ij, ik -> ijk', Y_og, sense_features)\n",
    "        Y_og_norm = tf.linalg.diag_part(tf.matmul(Y_og, Y_og, transpose_b = True), k = 0)\n",
    "        sense_norm = tf.linalg.diag_part(tf.matmul(sense_features, sense_features, transpose_b = True), k = 0)\n",
    "        norm = Y_og_norm * tf.cast(sense_norm, tf.float32)\n",
    "        D_og = tf.transpose(tf.transpose(sense_mat) / norm)\n",
    "\n",
    "\n",
    "        Y_plus = embed_plus\n",
    "        sense_mat = tf.einsum('ij, ik -> ijk', Y_plus, sense_features)\n",
    "        Y_plus_norm = tf.linalg.diag_part(tf.matmul(Y_plus, Y_plus, transpose_b = True), k = 0)\n",
    "        sense_norm = tf.linalg.diag_part(tf.matmul(sense_features, sense_features, transpose_b = True), k = 0)\n",
    "        norm = Y_plus_norm * tf.cast(sense_norm, tf.float32)\n",
    "        D_plus = tf.transpose(tf.transpose(sense_mat) / norm)\n",
    "\n",
    "        norm_og = [np.linalg.norm(D_og[node, :, :], ord = 'nuc') for node in range(len(graph))]\n",
    "        norm_plus = [np.linalg.norm(D_plus[node, :, :], ord = 'nuc') for node in range(len(graph))]\n",
    "        \n",
    "        try:\n",
    "            results[d]['norm_og'].append(norm_og)\n",
    "            results[d]['norm_plus'].append(norm_plus)\n",
    "            results[d]['explain_og_norm'].append(explain_og_norm)\n",
    "            results[d]['explain_plus_norm'].append(explain_plus_norm)\n",
    "            \n",
    "        except: \n",
    "            results[d]['norm_og'] = [norm_og]\n",
    "            results[d]['norm_plus'] = [norm_plus]\n",
    "            results[d]['explain_og_norm'] = [explain_og_norm]\n",
    "            results[d]['explain_plus_norm'] = [explain_plus_norm]\n",
    "            \n",
    "        results[d]['embed_og'] = embed_og\n",
    "        results[d]['embed_plus'] = embed_plus\n",
    "    \n",
    "    with open(outfile, 'wb') as file: \n",
    "        pkl.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af0218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711cef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './email.pkl'\n",
    "run_count = 1\n",
    "hyp_key = 'hyp_email'\n",
    "outfile = './email_line.pkl'\n",
    "\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-g\", \"--graph_path\", required = True, help = 'Path to an nx.Graph object stored as a .pkl file')\n",
    "# ap.add_argument(\"-r\", \"--run_count\", required = True, help = \"Number of iterations for the experiment\", default = 1)\n",
    "# ap.add_argument(\"-k\", \"--hyp_key\", required = True, help = \"Key to index the hyperparameter json file\")\n",
    "# ap.add_argument(\"-o\", \"--outfile\", required = True, help = \"File name to save results into\")\n",
    "\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "# filename = args['graph_path']\n",
    "# run_count = args['run_count']\n",
    "# hyp_key = args['hyp_key']\n",
    "# outfile = args['outfile']\n",
    "\n",
    "#################################\n",
    "######### Read In Graph #########\n",
    "#################################\n",
    "with open(filename, 'rb') as file: \n",
    "    graph_dict = pkl.load(file)\n",
    "    \n",
    "graph = nx.Graph(nx.to_numpy_array(graph_dict['graph']))    \n",
    "graph = nx.Graph(nx.to_numpy_array(graph))\n",
    "\n",
    "\n",
    "#################################\n",
    "#### Generate Sense Features ####\n",
    "#################################\n",
    "sense_feat_dict, sense_features = get_sense_features(graph, ppr_flag = 'std')\n",
    "\n",
    "uncorrelated_feats = ['Degree',\n",
    "                    'Clustering Coefficient',\n",
    "                    'Personalized Page Rank - Standard Deviation',\n",
    "                    'Average Neighbor Degree',\n",
    "                    'Average Neighbor Clustering',\n",
    "                    'Eccentricity',\n",
    "                    'Katz Centrality']\n",
    "sense_features = sense_features[:, [list(sense_feat_dict).index(feat) for feat in uncorrelated_feats]]\n",
    "sense_feat_dict = {feat : idx for idx, feat in enumerate(uncorrelated_feats)}\n",
    "\n",
    "#################################\n",
    "######## Hyperparameters ########\n",
    "#################################\n",
    "\n",
    "# Define static ones to override or read in from a file\n",
    "\n",
    "if hyp_key == '':\n",
    "    hyp = {'line' : {'alpha' : 0.1, \n",
    "                     'ortho' : 0, \n",
    "                     'sparse' : 0, \n",
    "                     'epochs' : 15, \n",
    "                     'batch_size' : 1024, \n",
    "                     'lr' : 1e-3}, \n",
    "\n",
    "          'line+xm' : {'alpha' : 100, \n",
    "                      'ortho' : 10, \n",
    "                      'sparse' : 10, \n",
    "                      'epochs' : 50, \n",
    "                      'batch_size' : 1024, \n",
    "                      'lr' : 5e-4}}\n",
    "else: \n",
    "    with open('scripts/hyp.json', 'r') as file: \n",
    "        hyp_file = json.load(file)\n",
    "        hyp = hyp_file[hyp_key]\n",
    "\n",
    "\n",
    "#################################\n",
    "######## Run Experiment #########\n",
    "#################################\n",
    "\n",
    "dimensions = [16, 32, 64, 256, 512]\n",
    "results = {d : {} for d in dimensions}\n",
    "\n",
    "for run_idx in tqdm(range(run_count)):\n",
    "    \n",
    "    for d in dimensions: \n",
    "    \n",
    "        # Embed \n",
    "        \n",
    "        # Standard LINE\n",
    "        line = LINE(graph, \n",
    "                embedding_size = d,\n",
    "                sense_features = sense_features,\n",
    "                alpha = hyp['line']['alpha'], \n",
    "                ortho = hyp['line']['ortho'], \n",
    "                sparse = hyp['line']['sparse'],\n",
    "                learning_rate =  hyp['line']['lr'],\n",
    "                order = 'second', \n",
    "                batch_size = hyp['line']['batch_size'])\n",
    "\n",
    "        history = line.train(epochs = hyp['line']['epochs'])\n",
    "\n",
    "        e = line.get_embeddings()\n",
    "        embed_og = np.array([e[node_name] for node_name in graph.nodes()])\n",
    "        embed_og = (embed_og - np.min(embed_og)) / np.ptp(embed_og)\n",
    "\n",
    "\n",
    "        feature_dict_og = find_feature_membership(input_embed = embed_og,\n",
    "                                                            embed_name = 'LINE',\n",
    "                                                            sense_features = sense_features,\n",
    "                                                            sense_feat_dict = sense_feat_dict,\n",
    "                                                            top_k = 8,\n",
    "                                                            solver = 'nmf')\n",
    "\n",
    "        explain_og = feature_dict_og['explain_norm']\n",
    "        error_og = sense_features * np.log((sense_features + 1e-10) / ((embed_og @ feature_dict_og['explain_norm']) + 1e-10)) - sense_features + (embed_og @ feature_dict_og['explain_norm'])\n",
    "        explain_og = (explain_og - np.min(explain_og)) / np.ptp(explain_og)\n",
    "        \n",
    "        # LINE+XM\n",
    "        \n",
    "        \n",
    "        # Generate Graph Explanations and Save\n",
    "        feature_dict_og = find_feature_membership(input_embed = embed_og,\n",
    "                                                    embed_name = 'SDNE',\n",
    "                                                    sense_features = sense_features,\n",
    "                                                    sense_feat_dict = sense_feat_dict,\n",
    "                                                    top_k = 8,\n",
    "                                                    solver = 'nmf')\n",
    "\n",
    "        explain_og = feature_dict_og['explain_norm']\n",
    "        explain_og = (explain_og - np.min(explain_og)) / np.ptp(explain_og)\n",
    "        explain_og_norm = np.linalg.norm(explain_og, ord = 'nuc')\n",
    "        \n",
    "        feature_dict_plus = find_feature_membership(input_embed = embed_plus,\n",
    "                                                            embed_name = 'SDNE+ Init',\n",
    "                                                            sense_features = sense_features,\n",
    "                                                            sense_feat_dict = sense_feat_dict,\n",
    "                                                            top_k = 8,\n",
    "                                                            solver = 'nmf')\n",
    "\n",
    "        explain_plus = feature_dict_plus['explain_norm']\n",
    "        explain_plus = (explain_plus - np.min(explain_plus)) / np.ptp(explain_plus)\n",
    "        explain_plus_norm = np.linalg.norm(explain_plus, ord = 'nuc')\n",
    "\n",
    "        # Generate Node Explanations\n",
    "        Y_og = embed_og\n",
    "        sense_mat = tf.einsum('ij, ik -> ijk', Y_og, sense_features)\n",
    "        Y_og_norm = tf.linalg.diag_part(tf.matmul(Y_og, Y_og, transpose_b = True), k = 0)\n",
    "        sense_norm = tf.linalg.diag_part(tf.matmul(sense_features, sense_features, transpose_b = True), k = 0)\n",
    "        norm = Y_og_norm * tf.cast(sense_norm, tf.float32)\n",
    "        D_og = tf.transpose(tf.transpose(sense_mat) / norm)\n",
    "\n",
    "\n",
    "        Y_plus = embed_plus\n",
    "        sense_mat = tf.einsum('ij, ik -> ijk', Y_plus, sense_features)\n",
    "        Y_plus_norm = tf.linalg.diag_part(tf.matmul(Y_plus, Y_plus, transpose_b = True), k = 0)\n",
    "        sense_norm = tf.linalg.diag_part(tf.matmul(sense_features, sense_features, transpose_b = True), k = 0)\n",
    "        norm = Y_plus_norm * tf.cast(sense_norm, tf.float32)\n",
    "        D_plus = tf.transpose(tf.transpose(sense_mat) / norm)\n",
    "\n",
    "        norm_og = [np.linalg.norm(D_og[node, :, :], ord = 'nuc') for node in range(len(graph))]\n",
    "        norm_plus = [np.linalg.norm(D_plus[node, :, :], ord = 'nuc') for node in range(len(graph))]\n",
    "        \n",
    "        try:\n",
    "            results[d]['norm_og'].append(norm_og)\n",
    "            results[d]['norm_plus'].append(norm_plus)\n",
    "            results[d]['explain_og_norm'].append(explain_og_norm)\n",
    "            results[d]['explain_plus_norm'].append(explain_plus_norm)\n",
    "            \n",
    "        except: \n",
    "            results[d]['norm_og'] = [norm_og]\n",
    "            results[d]['norm_plus'] = [norm_plus]\n",
    "            results[d]['explain_og_norm'] = [explain_og_norm]\n",
    "            results[d]['explain_plus_norm'] = [explain_plus_norm]\n",
    "            \n",
    "        results[d]['embed_og'] = embed_og\n",
    "        results[d]['embed_plus'] = embed_plus\n",
    "    \n",
    "    with open(outfile, 'wb') as file: \n",
    "        pkl.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7693011a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c88c6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
