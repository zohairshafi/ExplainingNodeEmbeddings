{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3d8518",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261b35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math \n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import numpy.random as rand\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "\n",
    "from scipy.sparse import linalg as spla\n",
    "from tensorflow.keras import layers, models, Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, Dropout, Concatenate, Embedding, Lambda, Reshape\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, History\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from collections import deque\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a546b51f",
   "metadata": {},
   "source": [
    "### LINE+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfa3d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nxgraph(graph):\n",
    "    node2idx = {}\n",
    "    idx2node = []\n",
    "    node_size = 0\n",
    "    for node in graph.nodes():\n",
    "        node2idx[node] = node_size\n",
    "        idx2node.append(node)\n",
    "        node_size += 1\n",
    "    return idx2node, node2idx\n",
    "\n",
    "\n",
    "def main_loss(alpha):\n",
    "    def line_loss(y_true, y_pred):\n",
    "        return alpha * -K.mean(K.log(K.sigmoid(y_true * y_pred)))\n",
    "    return line_loss\n",
    "\n",
    "def l_ortho_line(gamma):\n",
    "    \n",
    "    def loss_3rd(y_true, y_pred):\n",
    "        \n",
    "        E = y_pred\n",
    "        A = y_true\n",
    "        \n",
    "        batch_size = tf.cast(K.shape(A)[0], np.float32)\n",
    "    \n",
    "        return gamma * E / batch_size\n",
    "\n",
    "    \n",
    "    return loss_3rd\n",
    "\n",
    "def l_sparse_line(delta):\n",
    "    \n",
    "    def loss_4th(y_true, y_pred):\n",
    "        \n",
    "        E = y_pred\n",
    "        sense = y_true\n",
    "                \n",
    "        batch_size = tf.cast(K.shape(E)[0], np.float32)\n",
    "            \n",
    "        return delta * tf.reduce_sum(tf.norm(E, ord = 1, axis = 0)) / batch_size\n",
    "    \n",
    "    return loss_4th\n",
    "\n",
    "def create_model_line(num_nodes, embedding_size, sense_feat_size, order = 'second', batch_size = 128):\n",
    "\n",
    "    v_i = Input(shape = (1,))\n",
    "    v_j = Input(shape = (1,))\n",
    "    adj = Input(shape = (None, ))\n",
    "    sense_i = Input(shape = (sense_feat_size, ))\n",
    "    \n",
    "\n",
    "    first_emb = Embedding(num_nodes, embedding_size, name = 'first_emb')\n",
    "    second_emb = Embedding(num_nodes, embedding_size, name = 'second_emb')\n",
    "    context_emb = Embedding(num_nodes, embedding_size, name = 'context_emb')\n",
    "\n",
    "    v_i_emb = first_emb(v_i)\n",
    "    v_j_emb = first_emb(v_j)\n",
    "\n",
    "    v_i_emb_second = second_emb(v_i)\n",
    "    v_j_context_emb = context_emb(v_j)\n",
    "\n",
    "    ### First Embed ###\n",
    "    first = Lambda(lambda x: tf.reduce_sum(x[0] * x[1],\n",
    "                                               axis = -1,\n",
    "                                               keepdims = False),\n",
    "                       name = 'first_order')([v_i_emb, v_j_emb])\n",
    "    if order == 'first':\n",
    "    \n",
    "        first_embed = Reshape((embedding_size,), name = 'ortho_1')(v_i_emb)\n",
    "        sense_mat = tf.einsum('ij, ik -> ijk', first_embed, sense_i)\n",
    "        E = sense_mat\n",
    "        y_norm = tf.linalg.diag_part(tf.matmul(first_embed, first_embed, transpose_b = True), k = 0)\n",
    "        sense_norm = tf.linalg.diag_part(tf.matmul(sense_i, sense_i, transpose_b = True), k = 0)\n",
    "        norm = tf.multiply(y_norm, sense_norm)\n",
    "        E = tf.transpose(tf.transpose(E) / norm)\n",
    "\n",
    "    \n",
    "    ### Second Embed ###\n",
    "    second = Lambda(lambda x: tf.reduce_sum(x[0] * x[1],\n",
    "                                                axis = -1,\n",
    "                                                keepdims = False),\n",
    "                        name = 'second_order')([v_i_emb_second, v_j_context_emb])\n",
    "    if order == 'second':\n",
    "        \n",
    "        second_embed = Reshape((embedding_size,), name = 'ortho_2')(v_i_emb_second)\n",
    "\n",
    "        sense_mat = tf.einsum('ij, ik -> ijk', second_embed, sense_i)\n",
    "        E = sense_mat\n",
    "        y_norm = tf.linalg.diag_part(tf.matmul(second_embed, second_embed, transpose_b = True), k = 0)\n",
    "        sense_norm = tf.linalg.diag_part(tf.matmul(sense_i, sense_i, transpose_b = True), k = 0)\n",
    "        norm = tf.multiply(y_norm, sense_norm)\n",
    "        E = tf.transpose(tf.transpose(E) / norm)\n",
    "    \n",
    "    ### Loss Computations\n",
    "    E_t = tf.transpose(E, perm = [0, 2, 1])\n",
    "\n",
    "    E_1 = tf.einsum('aij, ajh -> aih', E, E_t)\n",
    "    E_1 = tf.reduce_sum(E_1)\n",
    "    \n",
    "    E_2 = tf.multiply(1.0, E, name = 'sparse_loss')\n",
    "    \n",
    "    ####\n",
    "\n",
    "    if order == 'first':\n",
    "        output_list = [first_embed, E_1, E_2]\n",
    "    \n",
    "    elif order == 'second':\n",
    "        output_list = [second_embed, E_1, E_2]\n",
    "    \n",
    "    else:\n",
    "        output_list = [first_embed, second_embed, [E_1, E_2], [E_1, E_2]]\n",
    "\n",
    "    model = Model(inputs = [v_i, v_j, adj, sense_i], outputs = output_list)\n",
    "\n",
    "    return model, {'first': first_emb, 'second': second_emb}\n",
    "\n",
    "\n",
    "class LINE:\n",
    "    def __init__(self, graph, sense_features, alpha, ortho, sparse, learning_rate, batch_size, embedding_size = 8, negative_ratio = 5, order = 'second',):\n",
    "        \"\"\"\n",
    "        :param graph:\n",
    "        :param embedding_size:\n",
    "        :param negative_ratio:\n",
    "        :param order: 'first','second','all'\n",
    "        \"\"\"\n",
    "        if order not in ['first', 'second', 'all']:\n",
    "            raise ValueError('mode must be first, second or all')\n",
    "\n",
    "        self.graph = graph\n",
    "        self.idx2node, self.node2idx = preprocess_nxgraph(graph)\n",
    "        self.use_alias = True\n",
    "\n",
    "        self.rep_size = embedding_size\n",
    "        self.order = order\n",
    "        self.sense_features = sense_features\n",
    "        self.sense_feat_size = self.sense_features.shape[1]\n",
    "        self.alpha = alpha\n",
    "        self.gamma = ortho\n",
    "        self.delta = sparse\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        self._embeddings = {}\n",
    "        self.negative_ratio = negative_ratio\n",
    "        self.order = order\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.node_size = graph.number_of_nodes()\n",
    "        self.edge_size = graph.number_of_edges()\n",
    "        self.samples_per_epoch = self.edge_size * (1 + negative_ratio)\n",
    "\n",
    "        self._gen_sampling_table()\n",
    "        self.reset_model()\n",
    "        \n",
    "        \n",
    "        self.A, self.L = self._create_A_L(\n",
    "            self.graph, self.node2idx)  # Adj Matrix, L Matrix\n",
    "\n",
    "    def reset_training_config(self, batch_size, times):\n",
    "        self.batch_size = batch_size\n",
    "        self.steps_per_epoch = (\n",
    "            (self.samples_per_epoch - 1) // self.batch_size + 1) * times\n",
    "\n",
    "    def reset_model(self, opt = 'adam'):\n",
    "\n",
    "        self.model, self.embedding_dict = create_model_line(self.node_size,\n",
    "                                                       self.rep_size, \n",
    "                                                       self.sense_feat_size,\n",
    "                                                       self.order, \n",
    "                                                       self.batch_size)\n",
    "        opt = Adam(learning_rate = self.lr, clipnorm = 0.5)\n",
    "        self.model.compile(opt, [main_loss(self.alpha), l_ortho_line(self.gamma), l_sparse_line(self.delta)])\n",
    "        self.batch_it = self.batch_iter(self.node2idx)\n",
    "\n",
    "    def _gen_sampling_table(self):\n",
    "\n",
    "        # create sampling table for vertex\n",
    "        power = 0.75\n",
    "        num_nodes = self.node_size\n",
    "        node_degree = np.zeros(num_nodes)  # out degree\n",
    "        node2idx = self.node2idx\n",
    "\n",
    "        for edge in self.graph.edges():\n",
    "            node_degree[node2idx[edge[0]]] += self.graph[edge[0]][edge[1]].get('weight', 1.0)\n",
    "\n",
    "        total_sum = sum([math.pow(node_degree[i], power)\n",
    "                         for i in range(num_nodes)])\n",
    "        norm_prob = [float(math.pow(node_degree[j], power)) /\n",
    "                     total_sum for j in range(num_nodes)]\n",
    "\n",
    "        self.node_accept, self.node_alias = create_alias_table(norm_prob)\n",
    "\n",
    "        # create sampling table for edge\n",
    "        numEdges = self.graph.number_of_edges()\n",
    "        total_sum = sum([self.graph[edge[0]][edge[1]].get('weight', 1.0)\n",
    "                         for edge in self.graph.edges()])\n",
    "        norm_prob = [self.graph[edge[0]][edge[1]].get('weight', 1.0) *\n",
    "                     numEdges / total_sum for edge in self.graph.edges()]\n",
    "\n",
    "        self.edge_accept, self.edge_alias = create_alias_table(norm_prob)\n",
    "\n",
    "    def batch_iter(self, node2idx):\n",
    "\n",
    "        edges = [(node2idx[x[0]], node2idx[x[1]]) for x in self.graph.edges()]\n",
    "\n",
    "        data_size = self.graph.number_of_edges()\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        # positive or negative mod\n",
    "        mod = 0\n",
    "        mod_size = 1 + self.negative_ratio\n",
    "        h = []\n",
    "        t = []\n",
    "        sign = 0\n",
    "        count = 0\n",
    "        start_index = 0\n",
    "        end_index = min(start_index + self.batch_size, data_size)\n",
    "        while True:\n",
    "            if mod == 0:\n",
    "\n",
    "                h = []\n",
    "                t = []\n",
    "                for i in range(start_index, end_index):\n",
    "                    if random.random() >= self.edge_accept[shuffle_indices[i]]:\n",
    "                        shuffle_indices[i] = self.edge_alias[shuffle_indices[i]]\n",
    "                    cur_h = edges[shuffle_indices[i]][0]\n",
    "                    cur_t = edges[shuffle_indices[i]][1]\n",
    "                    h.append(cur_h)\n",
    "                    t.append(cur_t)\n",
    "                sign = np.ones(len(h))\n",
    "            else:\n",
    "                sign = np.ones(len(h))*-1\n",
    "                t = []\n",
    "                for i in range(len(h)):\n",
    "\n",
    "                    t.append(alias_sample(\n",
    "                        self.node_accept, self.node_alias))\n",
    "                    \n",
    "            sense_feats = self.sense_features[np.array(h)]\n",
    "            adj = self.A[np.array(h), :]\n",
    "            adj = adj[:, np.array(h)].todense()\n",
    "            #assert adj.shape == (self.batch_size, self.batch_size)\n",
    "            \n",
    "            if self.order == 'all':\n",
    "                yield ([np.array(h), np.array(t), adj, sense_feats],\n",
    "                       [sign, sign, sense_feats, sense_feats])\n",
    "            else:\n",
    "                yield ([np.array(h), np.array(t), adj, sense_feats],\n",
    "                       [sign, sense_feats, sense_feats])\n",
    "            mod += 1\n",
    "            mod %= mod_size\n",
    "            if mod == 0:\n",
    "                start_index = end_index\n",
    "                end_index = min(start_index + self.batch_size, data_size)\n",
    "\n",
    "            if start_index >= data_size:\n",
    "                count += 1\n",
    "                mod = 0\n",
    "                h = []\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                start_index = 0\n",
    "                end_index = min(start_index + self.batch_size, data_size)\n",
    "\n",
    "    def get_embeddings(self,):\n",
    "        self._embeddings = {}\n",
    "        if self.order == 'first':\n",
    "            embeddings = self.embedding_dict['first'].get_weights()[0]\n",
    "        elif self.order == 'second':\n",
    "            embeddings = self.embedding_dict['second'].get_weights()[0]\n",
    "        else:\n",
    "            embeddings = np.hstack((self.embedding_dict['first'].get_weights()[\n",
    "                                   0], self.embedding_dict['second'].get_weights()[0]))\n",
    "        idx2node = self.idx2node\n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            self._embeddings[idx2node[i]] = embedding\n",
    "\n",
    "        return self._embeddings\n",
    "\n",
    "    def train(self, epochs = 1, initial_epoch = 0, verbose = 1, times = 1):\n",
    "        batch_size = self.batch_size\n",
    "        self.reset_training_config(batch_size, times)\n",
    "        hist = self.model.fit(self.batch_it,\n",
    "                                        epochs = epochs,\n",
    "                                        initial_epoch = initial_epoch,\n",
    "                                        steps_per_epoch = self.steps_per_epoch,\n",
    "                                        verbose = verbose)\n",
    "\n",
    "        return hist\n",
    "    \n",
    "    def _create_A_L(self, graph, node2idx):\n",
    "        node_size = graph.number_of_nodes()\n",
    "        A_data = []\n",
    "        A_row_index = []\n",
    "        A_col_index = []\n",
    "\n",
    "        for edge in graph.edges():\n",
    "            v1, v2 = edge\n",
    "            edge_weight = graph[v1][v2].get('weight', 1)\n",
    "\n",
    "            A_data.append(edge_weight)\n",
    "            A_row_index.append(node2idx[v1])\n",
    "            A_col_index.append(node2idx[v2])\n",
    "\n",
    "        A = sp.csr_matrix((A_data, (A_row_index, A_col_index)), shape = (node_size, node_size))\n",
    "        A_ = sp.csr_matrix((A_data + A_data, (A_row_index + A_col_index, A_col_index + A_row_index)),\n",
    "                           shape=(node_size, node_size))\n",
    "\n",
    "        D = sp.diags(A_.sum(axis=1).flatten().tolist()[0])\n",
    "        L = D - A_\n",
    "        return A, L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffedcab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alias_table(area_ratio):\n",
    "    \"\"\"\n",
    "    :param area_ratio: sum(area_ratio)=1\n",
    "    :return: accept,alias\n",
    "    \"\"\"\n",
    "    l = len(area_ratio)\n",
    "    accept, alias = [0] * l, [0] * l\n",
    "    small, large = [], []\n",
    "    area_ratio_ = np.array(area_ratio) * l\n",
    "    for i, prob in enumerate(area_ratio_):\n",
    "        if prob < 1.0:\n",
    "            small.append(i)\n",
    "        else:\n",
    "            large.append(i)\n",
    "\n",
    "    while small and large:\n",
    "        small_idx, large_idx = small.pop(), large.pop()\n",
    "        accept[small_idx] = area_ratio_[small_idx]\n",
    "        alias[small_idx] = large_idx\n",
    "        area_ratio_[large_idx] = area_ratio_[large_idx] - \\\n",
    "                                 (1 - area_ratio_[small_idx])\n",
    "        if area_ratio_[large_idx] < 1.0:\n",
    "            small.append(large_idx)\n",
    "        else:\n",
    "            large.append(large_idx)\n",
    "\n",
    "    while large:\n",
    "        large_idx = large.pop()\n",
    "        accept[large_idx] = 1\n",
    "    while small:\n",
    "        small_idx = small.pop()\n",
    "        accept[small_idx] = 1\n",
    "\n",
    "    return accept, alias\n",
    "\n",
    "\n",
    "def alias_sample(accept, alias):\n",
    "    \"\"\"\n",
    "    :param accept:\n",
    "    :param alias:\n",
    "    :return: sample index\n",
    "    \"\"\"\n",
    "    N = len(accept)\n",
    "    i = int(np.random.random() * N)\n",
    "    r = np.random.random()\n",
    "    if r < accept[i]:\n",
    "        return i\n",
    "    else:\n",
    "        return alias[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a12836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
