{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a67a5db",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370bf6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb\n",
    "%run SDNE+.ipynb\n",
    "%run LINE+.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec89555",
   "metadata": {},
   "source": [
    "### Link Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0ce20",
   "metadata": {},
   "source": [
    "#### Read In Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "print (\"Reading Graph...\", end = '\\r')\n",
    "with open('/scratch/shafi.z/foursquare.pkl', 'rb') as file: \n",
    "    graph_dict  = pkl.load(file)\n",
    "    graph = graph_dict['graph']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96077b72",
   "metadata": {},
   "source": [
    "#### Create Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17245523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide Edges\n",
    "# Generate network with hidden edges and ensure a connected network\n",
    "print (\"Creating Graph Copies...\", end = '\\r')\n",
    "graph_hidden_one = graph.copy()\n",
    "graph_hidden_two = graph.copy()\n",
    "graph_hidden_three = graph.copy()\n",
    "\n",
    "print (\"Computing MST...\", end = '\\r')\n",
    "ig = igraph.Graph([[e[0], e[1]] for e in nx.to_edgelist(graph)])\n",
    "mst = ig.spanning_tree()\n",
    "\n",
    "print (\"Computing Edges To Remove...\", end = '\\r')\n",
    "all_edges = ig.get_edgelist()\n",
    "edge_no_remove = mst.get_edgelist()\n",
    "removable_edges = list(set(all_edges).difference(set(edge_no_remove)))\n",
    "\n",
    "num_hidden_edges = int(0.6 * len(all_edges))\n",
    "hidden_edges = np.array(removable_edges)[np.random.choice(len(removable_edges), num_hidden_edges, replace = False)]\n",
    "\n",
    "print (\"Creating Train Graphs...\", end = '\\r')\n",
    "graph_hidden_one.remove_edges_from(hidden_edges[:int(hidden_edges.shape[0] / 3), :])\n",
    "graph_hidden_two.remove_edges_from(hidden_edges[int(hidden_edges.shape[0] / 3) : 2 * int(hidden_edges.shape[0] / 3), :])\n",
    "graph_hidden_three.remove_edges_from(hidden_edges[2 * int(hidden_edges.shape[0] / 3) : hidden_edges.shape[0], :])\n",
    "\n",
    "print (\"Running Sanity Checks...\", end = '\\r')\n",
    "hidden_edges_one = [(e[0], e[1]) for e in hidden_edges[:int(hidden_edges.shape[0] / 3)]]\n",
    "hidden_edges_two = [(e[0], e[1]) for e in hidden_edges[int(hidden_edges.shape[0] / 3) : 2 * int(hidden_edges.shape[0] / 3), :]]\n",
    "hidden_edges_three = [(e[0], e[1]) for e in hidden_edges[2 * int(hidden_edges.shape[0] / 3) : hidden_edges.shape[0], :]]\n",
    "\n",
    "print (\"Set One Intersection : \", len(set(list(graph_hidden_one.edges)).intersection(set(hidden_edges_one))))\n",
    "print (\"Set Two Intersection : \", len(set(list(graph_hidden_two.edges)).intersection(set(hidden_edges_two))))\n",
    "print (\"Set Three Intersection : \", len(set(list(graph_hidden_three.edges)).intersection(set(hidden_edges_three))))\n",
    "\n",
    "\n",
    "all_test_edges_pos = hidden_edges_one + hidden_edges_two + hidden_edges_three\n",
    "ig = igraph.Graph([[e[0], e[1]] for e in nx.to_edgelist(graph)])\n",
    "all_edges = set(ig.get_edgelist())\n",
    "neg_indices = np.where(nx.to_numpy_array(graph) == 0)\n",
    "\n",
    "neg_edges = set()\n",
    "for _ in tqdm(range(9 * len(all_test_edges_pos))):\n",
    "    \n",
    "    idx = np.random.randint(neg_indices[0].shape[0])\n",
    "    neg_edges.add((neg_indices[0][idx], neg_indices[1][idx]))\n",
    "    \n",
    "neg_edges = list(neg_edges)\n",
    "\n",
    "ig_1 = igraph.Graph([[e[0], e[1]] for e in nx.to_edgelist(graph_hidden_one)])\n",
    "ig_1_edges = ig_1.get_edgelist()\n",
    "np.random.shuffle(ig_1_edges)\n",
    "\n",
    "train_1 = ig_1_edges[:2 * len(hidden_edges_one)]\n",
    "train_1_neg = neg_edges[:2 * len(hidden_edges_one)]\n",
    "\n",
    "ig_2 = igraph.Graph([[e[0], e[1]] for e in nx.to_edgelist(graph_hidden_two)])\n",
    "ig_2_edges = ig_2.get_edgelist()\n",
    "np.random.shuffle(ig_2_edges)\n",
    "\n",
    "train_2 = ig_2_edges[:2 * len(hidden_edges_two)]\n",
    "train_2_neg = neg_edges[2 * len(hidden_edges_one) : 4 * len(hidden_edges_one)]\n",
    "\n",
    "ig_3 = igraph.Graph([[e[0], e[1]] for e in nx.to_edgelist(graph_hidden_three)])\n",
    "ig_3_edges = ig_3.get_edgelist()\n",
    "np.random.shuffle(ig_3_edges)\n",
    "\n",
    "train_3 = ig_3_edges[:2 * len(hidden_edges_three)]\n",
    "train_3_neg = neg_edges[4 * len(hidden_edges_one) : 6 * len(hidden_edges_one)]\n",
    "\n",
    "test_1_neg = neg_edges[6 * len(hidden_edges_one) : 7 * len(hidden_edges_one)]\n",
    "test_2_neg = neg_edges[7 * len(hidden_edges_one) : 8 * len(hidden_edges_one)]\n",
    "test_3_neg = neg_edges[8 * len(hidden_edges_one) : 9 * len(hidden_edges_one)]\n",
    "\n",
    "intersection_1 = set(train_1 + train_1_neg).intersection(test_1_neg + hidden_edges_one)\n",
    "intersection_2 = set(train_2 + train_2_neg).intersection(test_2_neg + hidden_edges_two)\n",
    "intersection_3 = set(train_3 + train_3_neg).intersection(test_3_neg + hidden_edges_three)\n",
    "\n",
    "\n",
    "train_1 = [(x, y) for (x, y) in train_1 if (x, y) not in intersection_1]\n",
    "train_1_neg = [(x, y) for (x, y) in train_1_neg if (x, y) not in intersection_1]\n",
    "\n",
    "train_2 = [(x, y) for (x, y) in train_2 if (x, y) not in intersection_2]\n",
    "train_2_neg = [(x, y) for (x, y) in train_2_neg if (x, y) not in intersection_2]\n",
    "\n",
    "train_3 = [(x, y) for (x, y) in train_3 if (x, y) not in intersection_3]\n",
    "train_3_neg = [(x, y) for (x, y) in train_3_neg if (x, y) not in intersection_3]\n",
    "\n",
    "\n",
    "print (\"Train Set 1 - Positive : \", len(train_1))\n",
    "print (\"Train Set 1 - Negative : \", len(train_1_neg))\n",
    "print (\"Test Set 1 - Positive : \", len(hidden_edges_one))\n",
    "print (\"Test Set 1 - Negative : \", len(test_1_neg))\n",
    "print (\"Intersection : \", set(train_1 + train_1_neg).intersection(test_1_neg + hidden_edges_one))\n",
    "print ()\n",
    "\n",
    "print (\"Train Set 2 - Positive : \", len(train_2))\n",
    "print (\"Train Set 2 - Negative : \", len(train_2_neg))\n",
    "print (\"Test Set 2 - Positive : \", len(hidden_edges_two))\n",
    "print (\"Test Set 2 - Negative : \", len(test_2_neg))\n",
    "print (\"Intersection : \", set(train_2 + train_2_neg).intersection(test_2_neg + hidden_edges_two))\n",
    "print ()\n",
    "\n",
    "\n",
    "print (\"Train Set 3 - Positive : \", len(train_3))\n",
    "print (\"Train Set 3 - Negative : \", len(train_3_neg))\n",
    "print (\"Test Set 3 - Positive : \", len(hidden_edges_three))\n",
    "print (\"Test Set 3 - Negative : \", len(test_3_neg))\n",
    "print (\"Intersection : \", set(train_3 + train_3_neg).intersection(test_3_neg + hidden_edges_three))\n",
    "\n",
    "graph_dict = {'graph' : graph, \n",
    "              'graph_hidden_1' : graph_hidden_one, \n",
    "              'graph_hidden_2' : graph_hidden_two, \n",
    "              'graph_hidden_3' : graph_hidden_three,\n",
    "              'train_1' : train_1,\n",
    "              'train_2' : train_2,\n",
    "              'train_3' : train_3,\n",
    "              'train_1_neg' : train_1_neg,\n",
    "              'train_2_neg' : train_2_neg,\n",
    "              'train_3_neg' : train_3_neg,\n",
    "              'test_1' : hidden_edges_one, \n",
    "              'test_2' : hidden_edges_two, \n",
    "              'test_3' : hidden_edges_three,\n",
    "              'test_1_neg' : test_1_neg,\n",
    "              'test_2_neg' : test_2_neg,\n",
    "              'test_3_neg' : test_3_neg,\n",
    "        }\n",
    "\n",
    "with open('/scratch/shafi.z/foursquare.pkl', 'wb') as file: \n",
    "    pkl.dump(graph_dict, file)\n",
    "\n",
    "\n",
    "\n",
    "print (\"Computing Sense Features For Fold 1...\", end = '\\r')\n",
    "sense_feat_dict_one, sense_features_one = get_sense_features(graph_hidden_one, ppr_flag = 'std')\n",
    "\n",
    "graph_dict = {'graph' : graph, \n",
    "              'graph_hidden_1' : graph_hidden_one, \n",
    "              'graph_hidden_2' : graph_hidden_two, \n",
    "              'graph_hidden_3' : graph_hidden_three,\n",
    "              'train_1' : train_1,\n",
    "              'train_2' : train_2,\n",
    "              'train_3' : train_3,\n",
    "              'train_1_neg' : train_1_neg,\n",
    "              'train_2_neg' : train_2_neg,\n",
    "              'train_3_neg' : train_3_neg,\n",
    "              'test_1' : hidden_edges_one, \n",
    "              'test_2' : hidden_edges_two, \n",
    "              'test_3' : hidden_edges_three,\n",
    "              'test_1_neg' : test_1_neg,\n",
    "              'test_2_neg' : test_2_neg,\n",
    "              'test_3_neg' : test_3_neg,\n",
    "              'sense_feat_dict' : sense_feat_dict_one,\n",
    "              'sense_features_1' : sense_features_one, \n",
    "        }\n",
    "\n",
    "with open('/scratch/shafi.z/foursquare.pkl', 'wb') as file: \n",
    "    pkl.dump(graph_dict, file)\n",
    "\n",
    "print (\"Computing Sense Features For Fold 2...\", end = '\\r')\n",
    "sense_feat_dict_two, sense_features_two = get_sense_features(graph_hidden_two, ppr_flag = 'std')\n",
    "\n",
    "graph_dict = {'graph' : graph, \n",
    "              'graph_hidden_1' : graph_hidden_one, \n",
    "              'graph_hidden_2' : graph_hidden_two, \n",
    "              'graph_hidden_3' : graph_hidden_three,\n",
    "              'train_1' : train_1,\n",
    "              'train_2' : train_2,\n",
    "              'train_3' : train_3,\n",
    "              'train_1_neg' : train_1_neg,\n",
    "              'train_2_neg' : train_2_neg,\n",
    "              'train_3_neg' : train_3_neg,\n",
    "              'test_1' : hidden_edges_one, \n",
    "              'test_2' : hidden_edges_two, \n",
    "              'test_3' : hidden_edges_three,\n",
    "              'test_1_neg' : test_1_neg,\n",
    "              'test_2_neg' : test_2_neg,\n",
    "              'test_3_neg' : test_3_neg,\n",
    "              'sense_feat_dict' : sense_feat_dict_one,\n",
    "              'sense_features_1' : sense_features_one, \n",
    "              'sense_features_2' : sense_features_two, \n",
    "        }\n",
    "\n",
    "with open('/scratch/shafi.z/email_pos.pkl', 'wb') as file: \n",
    "    pkl.dump(graph_dict, file)\n",
    "\n",
    "print (\"Computing Sense Features For Fold 3...\", end = '\\r')\n",
    "sense_feat_dict_three, sense_features_three = get_sense_features(graph_hidden_three, ppr_flag = 'std')\n",
    "\n",
    "graph_dict = {'graph' : graph, \n",
    "              'graph_hidden_1' : graph_hidden_one, \n",
    "              'graph_hidden_2' : graph_hidden_two, \n",
    "              'graph_hidden_3' : graph_hidden_three,\n",
    "              'train_1' : train_1,\n",
    "              'train_2' : train_2,\n",
    "              'train_3' : train_3,\n",
    "              'train_1_neg' : train_1_neg,\n",
    "              'train_2_neg' : train_2_neg,\n",
    "              'train_3_neg' : train_3_neg,\n",
    "              'test_1' : hidden_edges_one, \n",
    "              'test_2' : hidden_edges_two, \n",
    "              'test_3' : hidden_edges_three,\n",
    "              'test_1_neg' : test_1_neg,\n",
    "              'test_2_neg' : test_2_neg,\n",
    "              'test_3_neg' : test_3_neg,\n",
    "              'sense_feat_dict' : sense_feat_dict_one,\n",
    "              'sense_features_1' : sense_features_one, \n",
    "              'sense_features_2' : sense_features_two, \n",
    "              'sense_features_3' : sense_features_three,\n",
    "        }\n",
    "\n",
    "with open('/scratch/shafi.z/email_pos.pkl', 'wb') as file: \n",
    "    pkl.dump(graph_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac08732",
   "metadata": {},
   "source": [
    "#### Embed and Predict - SDNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55cb5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in tqdm(range(1, 4)):\n",
    "    \n",
    "    graph = graph_dict['graph_hidden_' + str(idx)]\n",
    "    sense_feat_dict = graph_dict['sense_feat_dict']\n",
    "    sense_features = graph_dict['sense_features_' + str(idx)]\n",
    "    \n",
    "    \n",
    "    train = graph_dict['train_' + str(idx)] \n",
    "    train_neg = graph_dict['train_' + str(idx) + '_neg'] \n",
    "    test_pos = graph_dict['test_' + str(idx)]\n",
    "    test_neg = graph_dict['test_' + str(idx) + '_neg'] \n",
    "\n",
    "    \n",
    "    \n",
    "    # Run SDNE \n",
    "    sdne = SDNE_plus(graph, \n",
    "                      hidden_size = [64, 128], \n",
    "                      lr = 1e-3,\n",
    "                      sense_features = sense_features.astype(np.float32),\n",
    "                      alpha = 0.1, \n",
    "                      beta = 10, \n",
    "                      gamma = 0, \n",
    "                      delta = 0)\n",
    "\n",
    "    history = sdne.train(epochs = 50, batch_size = 10000)\n",
    "    print (\"Training Complete\")\n",
    "    embed = sdne.get_embeddings()\n",
    "    embedding = np.array([embed[node_name] for node_name in graph.nodes()])\n",
    "    print (\"Embeddings Generated\")\n",
    "    \n",
    "    sdne.model.save_weights('/scratch/shafi.z/foursquare_' + str(idx) + '_.model')\n",
    "    with open('/scratch/shafi.z/foursquare_embed_' + str(idx) + '_.pkl', 'wb') as file: \n",
    "        pkl.dump(embedding, file)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Run SDNE+ Init\n",
    "    sdne_plus_init = SDNE_plus(graph, \n",
    "                              hidden_size = [64, 128], \n",
    "                              lr = 1e-3,\n",
    "                              sense_features = sense_features.astype(np.float32),\n",
    "                              alpha = 5, \n",
    "                              beta = 5, \n",
    "                              gamma = 10, \n",
    "                              delta = 5)\n",
    "    \n",
    "    \n",
    "    sdne_plus_init.model.load_weights('/scratch/shafi.z/foursquare_' + str(idx) + '_.model')\n",
    "    \n",
    "    # sdne_plus_init.model.set_weights(sdne.model.get_weights())\n",
    "    history_plus_init = sdne_plus_init.train(epochs = 50, batch_size = 10000)\n",
    "    embed_plus = sdne_plus_init.get_embeddings()\n",
    "    embedding_plus_init = np.array([embed_plus[node_name] for node_name in graph.nodes()])\n",
    "    \n",
    "    sdne_plus_init.model.save_weights('/scratch/shafi.z/foursquare_plsu_' + str(idx) + '_.model')\n",
    "    with open('/scratch/shafi.z/foursquare_embed_plus_' + str(idx) + '_.pkl', 'wb') as file: \n",
    "        pkl.dump(embedding_plus_init, file)\n",
    "    \n",
    "    \n",
    "    with open('/scratch/shafi.z/foursquare_embed_' + str(idx) + '_.pkl', 'rb') as file: \n",
    "        embedding = pkl.load(file)\n",
    "        \n",
    "    with open('/scratch/shafi.z/foursquare_embed_plus_' + str(idx) + '_.pkl', 'rb') as file: \n",
    "        embedding_plus_init = pkl.load(file)\n",
    "    # Compare\n",
    "\n",
    "    feature_dict = find_feature_membership(input_embed = embedding,\n",
    "                                           embed_name = 'SDNE',\n",
    "                                           sense_features = sense_features,\n",
    "                                           sense_feat_dict = sense_feat_dict,\n",
    "                                           top_k = 8,\n",
    "                                           solver = 'nmf')\n",
    "\n",
    "    feature_dict_plus_init = find_feature_membership(input_embed = embedding_plus_init,\n",
    "                                                     embed_name = 'SDNE+ Init',\n",
    "                                                     sense_features = sense_features,\n",
    "                                                     sense_feat_dict = sense_feat_dict,\n",
    "                                                     top_k = 8,\n",
    "                                                     solver = 'nmf')\n",
    "    \n",
    "    # Run link prediction models\n",
    "    sdne_results = get_embed_perf(input_embed = embedding,\n",
    "                             input_dict = feature_dict,\n",
    "                             data = None, \n",
    "                             labels = None, \n",
    "                             graph = graph, \n",
    "                             hidden_edges = test_pos,\n",
    "                                  train_set = train,\n",
    "                                  train_set_neg = train_neg,\n",
    "                                  test_set = test_pos,\n",
    "                                  test_set_neg = test_neg, \n",
    "                                    epochs = 100)\n",
    "\n",
    "\n",
    "    sdne_plus_init_results = get_embed_perf(input_embed = embedding_plus_init,\n",
    "                                           input_dict = feature_dict_plus_init,\n",
    "                                           data = None, \n",
    "                                           labels = None, \n",
    "                                           graph = graph, \n",
    "                                           hidden_edges = test_pos,\n",
    "                                            train_set = train,\n",
    "                                            train_set_neg = train_neg,\n",
    "                                            test_set = test_pos,\n",
    "                                            test_set_neg = test_neg, \n",
    "                                            epochs = 100)\n",
    "    \n",
    "    \n",
    "    # Sum of Variances\n",
    "    variance = np.sum(np.square(np.std(feature_dict['explain_norm'], axis = 1)))\n",
    "    variance_plus_init = np.sum(np.square(np.std(feature_dict_plus_init['explain_norm'], axis = 1)))\n",
    "    \n",
    "    # Other Metrics\n",
    "    e = feature_dict['explain_norm']\n",
    "    e_plus_init = feature_dict_plus_init['explain_norm']\n",
    "\n",
    "    perc_zero = len(np.where(e == 0)[0]) / np.product(e.shape)\n",
    "    perc_zero_plus_init = len(np.where(e_plus_init == 0)[0]) / np.product(e_plus_init.shape)\n",
    "\n",
    "    ortho = np.sum(e @ e.T)\n",
    "    ortho_plus_init = np.sum(e_plus_init @ e_plus_init.T)\n",
    "\n",
    "    # Put into pretty dataframes\n",
    "    info = pd.DataFrame([[variance_plus_init, variance,], [perc_zero_plus_init, perc_zero], [ortho_plus_init, ortho]])\n",
    "    info.index = ['Sum Of Variances', 'Percentage of Zero Entries (Sparsity)', 'Orthogonality']\n",
    "    info.columns = ['SDNE+ Init', 'SDNE']\n",
    "\n",
    "    results = pd.concat([sdne_plus_init_results.T, sdne_results.T])\n",
    "    results.index = ['SDNE+ Init', 'SDNE']\n",
    "    results = results.T\n",
    "    results = results.append(info)\n",
    "\n",
    "    return_dict = {'graph' : graph_dict['graph'], \n",
    "                   'graph_hidden' : graph_hidden,\n",
    "                   'hidden_edges' : hidden_edges,\n",
    "                   'results' : results, \n",
    "                   'embedding' : embedding,\n",
    "                   'embedding_plus_init' : embedding_plus_init,\n",
    "                   'sense_feat_dict' : sense_feat_dict,\n",
    "                   'sense_features' : sense_features,\n",
    "                   'feature_dict' : feature_dict, \n",
    "                   'feature_dict_plus_init' : feature_dict_plus_init,\n",
    "                   'history' : history.history, \n",
    "                   'history_plus_init' : history_plus_init.history,}\n",
    "\n",
    "    with open('/scratch/shafi.z/foursquare_cv_linkpred_' + str(idx) + '_.pkl', 'wb') as file:\n",
    "        pkl.dump(return_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6e7c48",
   "metadata": {},
   "source": [
    "#### EMbed and Predict - LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463003d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in tqdm(range(1, 4)):\n",
    "    \n",
    "#     graph = graph_dict['graph_hidden_' + str(idx)]\n",
    "#     sense_feat_dict = graph_dict['sense_feat_dict']\n",
    "#     sense_features = graph_dict['sense_features_' + str(idx)]\n",
    "    \n",
    "    \n",
    "#     train = graph_dict['train_' + str(idx)] \n",
    "#     train_neg = graph_dict['train_' + str(idx) + '_neg'] \n",
    "#     test_pos = graph_dict['test_' + str(idx)]\n",
    "#     test_neg = graph_dict['test_' + str(idx) + '_neg'] \n",
    "\n",
    "    \n",
    "    \n",
    "#     # Run LINE \n",
    "#     sdne = SDNE_plus(graph, \n",
    "#                       hidden_size = [64, 128], \n",
    "#                       lr = 1e-3,\n",
    "#                       sense_features = sense_features.astype(np.float32),\n",
    "#                       alpha = 0.1, \n",
    "#                       beta = 10, \n",
    "#                       gamma = 0, \n",
    "#                       delta = 0)\n",
    "\n",
    "#     history = sdne.train(epochs = 50, batch_size = 10000)\n",
    "#     print (\"Training Complete\")\n",
    "#     embed = sdne.get_embeddings()\n",
    "#     embedding = np.array([embed[node_name] for node_name in graph.nodes()])\n",
    "#     print (\"Embeddings Generated\")\n",
    "    \n",
    "#     sdne.model.save_weights('/scratch/shafi.z/foursquare_' + str(idx) + '_.model')\n",
    "#     with open('/scratch/shafi.z/foursquare_embed_' + str(idx) + '_.pkl', 'wb') as file: \n",
    "#         pkl.dump(embedding, file)\n",
    "\n",
    "    \n",
    "    \n",
    "#     # Run SDNE+ Init\n",
    "#     sdne_plus_init = SDNE_plus(graph, \n",
    "#                               hidden_size = [64, 128], \n",
    "#                               lr = 1e-3,\n",
    "#                               sense_features = sense_features.astype(np.float32),\n",
    "#                               alpha = 5, \n",
    "#                               beta = 5, \n",
    "#                               gamma = 10, \n",
    "#                               delta = 5)\n",
    "    \n",
    "    \n",
    "#     sdne_plus_init.model.load_weights('/scratch/shafi.z/foursquare_' + str(idx) + '_.model')\n",
    "    \n",
    "#     # sdne_plus_init.model.set_weights(sdne.model.get_weights())\n",
    "#     history_plus_init = sdne_plus_init.train(epochs = 50, batch_size = 10000)\n",
    "#     embed_plus = sdne_plus_init.get_embeddings()\n",
    "#     embedding_plus_init = np.array([embed_plus[node_name] for node_name in graph.nodes()])\n",
    "    \n",
    "#     sdne_plus_init.model.save_weights('/scratch/shafi.z/foursquare_plsu_' + str(idx) + '_.model')\n",
    "#     with open('/scratch/shafi.z/foursquare_embed_plus_' + str(idx) + '_.pkl', 'wb') as file: \n",
    "#         pkl.dump(embedding_plus_init, file)\n",
    "    \n",
    "    \n",
    "#     with open('/scratch/shafi.z/foursquare_embed_' + str(idx) + '_.pkl', 'rb') as file: \n",
    "#         embedding = pkl.load(file)\n",
    "        \n",
    "#     with open('/scratch/shafi.z/foursquare_embed_plus_' + str(idx) + '_.pkl', 'rb') as file: \n",
    "#         embedding_plus_init = pkl.load(file)\n",
    "#     # Compare\n",
    "\n",
    "#     feature_dict = find_feature_membership(input_embed = embedding,\n",
    "#                                            embed_name = 'SDNE',\n",
    "#                                            sense_features = sense_features,\n",
    "#                                            sense_feat_dict = sense_feat_dict,\n",
    "#                                            top_k = 8,\n",
    "#                                            solver = 'nmf')\n",
    "\n",
    "#     feature_dict_plus_init = find_feature_membership(input_embed = embedding_plus_init,\n",
    "#                                                      embed_name = 'SDNE+ Init',\n",
    "#                                                      sense_features = sense_features,\n",
    "#                                                      sense_feat_dict = sense_feat_dict,\n",
    "#                                                      top_k = 8,\n",
    "#                                                      solver = 'nmf')\n",
    "    \n",
    "#     # Run link prediction models\n",
    "#     sdne_results = get_embed_perf(input_embed = embedding,\n",
    "#                              input_dict = feature_dict,\n",
    "#                              data = None, \n",
    "#                              labels = None, \n",
    "#                              graph = graph, \n",
    "#                              hidden_edges = test_pos,\n",
    "#                                   train_set = train,\n",
    "#                                   train_set_neg = train_neg,\n",
    "#                                   test_set = test_pos,\n",
    "#                                   test_set_neg = test_neg, \n",
    "#                                     epochs = 100)\n",
    "\n",
    "\n",
    "#     sdne_plus_init_results = get_embed_perf(input_embed = embedding_plus_init,\n",
    "#                                            input_dict = feature_dict_plus_init,\n",
    "#                                            data = None, \n",
    "#                                            labels = None, \n",
    "#                                            graph = graph, \n",
    "#                                            hidden_edges = test_pos,\n",
    "#                                             train_set = train,\n",
    "#                                             train_set_neg = train_neg,\n",
    "#                                             test_set = test_pos,\n",
    "#                                             test_set_neg = test_neg, \n",
    "#                                             epochs = 100)\n",
    "    \n",
    "    \n",
    "#     # Sum of Variances\n",
    "#     variance = np.sum(np.square(np.std(feature_dict['explain_norm'], axis = 1)))\n",
    "#     variance_plus_init = np.sum(np.square(np.std(feature_dict_plus_init['explain_norm'], axis = 1)))\n",
    "    \n",
    "#     # Other Metrics\n",
    "#     e = feature_dict['explain_norm']\n",
    "#     e_plus_init = feature_dict_plus_init['explain_norm']\n",
    "\n",
    "#     perc_zero = len(np.where(e == 0)[0]) / np.product(e.shape)\n",
    "#     perc_zero_plus_init = len(np.where(e_plus_init == 0)[0]) / np.product(e_plus_init.shape)\n",
    "\n",
    "#     ortho = np.sum(e @ e.T)\n",
    "#     ortho_plus_init = np.sum(e_plus_init @ e_plus_init.T)\n",
    "\n",
    "#     # Put into pretty dataframes\n",
    "#     info = pd.DataFrame([[variance_plus_init, variance,], [perc_zero_plus_init, perc_zero], [ortho_plus_init, ortho]])\n",
    "#     info.index = ['Sum Of Variances', 'Percentage of Zero Entries (Sparsity)', 'Orthogonality']\n",
    "#     info.columns = ['SDNE+ Init', 'SDNE']\n",
    "\n",
    "#     results = pd.concat([sdne_plus_init_results.T, sdne_results.T])\n",
    "#     results.index = ['SDNE+ Init', 'SDNE']\n",
    "#     results = results.T\n",
    "#     results = results.append(info)\n",
    "\n",
    "#     return_dict = {'graph' : graph_dict['graph'], \n",
    "#                    'graph_hidden' : graph_hidden,\n",
    "#                    'hidden_edges' : hidden_edges,\n",
    "#                    'results' : results, \n",
    "#                    'embedding' : embedding,\n",
    "#                    'embedding_plus_init' : embedding_plus_init,\n",
    "#                    'sense_feat_dict' : sense_feat_dict,\n",
    "#                    'sense_features' : sense_features,\n",
    "#                    'feature_dict' : feature_dict, \n",
    "#                    'feature_dict_plus_init' : feature_dict_plus_init,\n",
    "#                    'history' : history.history, \n",
    "#                    'history_plus_init' : history_plus_init.history,}\n",
    "\n",
    "#     with open('/scratch/shafi.z/foursquare_cv_linkpred_' + str(idx) + '_.pkl', 'wb') as file:\n",
    "#         pkl.dump(return_dict, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
