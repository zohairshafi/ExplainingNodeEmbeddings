{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb743bf9",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "148eef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from scripts.utils import *\n",
    "\n",
    "from GMI.models import GMI, LogReg\n",
    "from GMI.utils import process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e6d23",
   "metadata": {},
   "source": [
    "### Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d985ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbedder:\n",
    "    def __init__(self, graph, embed_shape = (128,)):\n",
    "        self.embed(graph)\n",
    "        self.E = list(graph.edges())\n",
    "        self.graph = graph\n",
    "        self.embed_shape = embed_shape\n",
    "    \n",
    "    def embed(self, graph):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        raise NotImplementedError\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b92c330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMIEmbedding(BaseEmbedder):\n",
    "    def __init__(self, embed_dim = 64, graph = None, feature_matrix = None, use_xm = False, debug = False, batch_size = 1, nb_epochs = 500, patience = 20, ortho_ = 0.1, sparse_ = 0.1, lr = 1e-3, l2_coef = 0.0, drop_prob = 0.0, sparse = True, nonlinearity = 'prelu', alpha = 0.8, beta = 1.0, gamma = 1.0, negative_num = 5, epoch_flag = 20, model_name = 'test'):\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Training Params\n",
    "        self.graph = graph\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.patience = patience\n",
    "        self.lr = lr\n",
    "        self.l2_coef = l2_coef\n",
    "        self.feature_matrix = feature_matrix\n",
    "        self.drop_prob = drop_prob\n",
    "        self.hid_units = embed_dim\n",
    "        self.sparse = sparse\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.use_xm = use_xm\n",
    "        self.ortho_ = ortho_\n",
    "        self.sparse_ = sparse_\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.negative_num = negative_num\n",
    "        self.epoch_flag = epoch_flag\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        self.time_per_epoch = None\n",
    "        \n",
    "        if graph is not None:\n",
    "            self.embed()\n",
    "        else:\n",
    "            self.graph = None\n",
    "    \n",
    "    def embed(self):\n",
    "\n",
    "        ####\n",
    "        if self.feature_matrix is None:\n",
    "            feature_matrix = np.identity(len(self.graph))\n",
    "        else: \n",
    "            feature_matrix = self.feature_matrix\n",
    "\n",
    "        adj_ori = nx.to_scipy_sparse_array(graph)\n",
    "        features = sp.lil_matrix(feature_matrix)\n",
    "        features, _ = process.preprocess_features(features)\n",
    "\n",
    "        nb_nodes = features.shape[0]\n",
    "        ft_size = features.shape[1]\n",
    "        \n",
    "        adj = process.normalize_adj(adj_ori + sp.eye(adj_ori.shape[0]))\n",
    "\n",
    "        if self.sparse:\n",
    "            sp_adj = process.sparse_mx_to_torch_sparse_tensor(adj)\n",
    "        else:\n",
    "            adj = (adj + sp.eye(adj.shape[0])).todense()\n",
    "\n",
    "        features = torch.FloatTensor(features[np.newaxis])\n",
    "        if not self.sparse:\n",
    "            adj = torch.FloatTensor(adj[np.newaxis])\n",
    "\n",
    "        if self.feature_matrix is not None: \n",
    "            sense_features = torch.FloatTensor(self.feature_matrix)\n",
    "\n",
    "        model = GMI(ft_size, self.hid_units, self.nonlinearity)\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr = self.lr, weight_decay = self.l2_coef)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "            features = features.cuda()\n",
    "            sp_adj = sp_adj.cuda()\n",
    "            \n",
    "        b_xent = nn.BCEWithLogitsLoss()\n",
    "        xent = nn.CrossEntropyLoss()\n",
    "        cnt_wait = 0\n",
    "        best = 1e9\n",
    "        best_t = 0\n",
    "        \n",
    "        \n",
    "        adj_dense = adj_ori.toarray()\n",
    "        adj_target = adj_dense + np.eye(adj_dense.shape[0])\n",
    "        adj_row_avg = 1.0 / np.sum(adj_dense, axis = 1)\n",
    "        adj_row_avg[np.isnan(adj_row_avg)] = 0.0\n",
    "        adj_row_avg[np.isinf(adj_row_avg)] = 0.0\n",
    "        adj_dense = adj_dense * 1.0\n",
    "        \n",
    "        for i in range(adj_ori.shape[0]):\n",
    "            adj_dense[i] = adj_dense[i] * adj_row_avg[i]\n",
    "        adj_ori = sp.csr_matrix(adj_dense, dtype = np.float32)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for epoch in tqdm(range(self.nb_epochs)):\n",
    "            model.train()\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            res = model(features, adj_ori, self.negative_num, sp_adj, None, None) \n",
    "            \n",
    "            if self.use_xm == True and feature_matrix is not None:\n",
    "                \n",
    "                start_idx = 0\n",
    "                loop = True\n",
    "                \n",
    "                ortho_loss = 0\n",
    "                sparse_loss = 0\n",
    "                xm_batch_size = 128\n",
    "                \n",
    "                sf = sense_features\n",
    "                embeds = model.embed(features, sp_adj)\n",
    "                                \n",
    "                while loop:\n",
    "                    end_idx = start_idx + xm_batch_size\n",
    "                    if end_idx > len(self.graph):\n",
    "                        loop = False\n",
    "                        end_idx = len(self.graph)\n",
    "                        \n",
    "                    \n",
    "                    sf = sense_features[start_idx : end_idx]\n",
    "                    embeds_ = torch.squeeze(embeds)[start_idx : end_idx]\n",
    "                    \n",
    "                    \n",
    "                    sense_mat = torch.einsum('ij, ik -> ijk', embeds_, sf)\n",
    "                    E = sense_mat\n",
    "                    y_norm = torch.diagonal(torch.matmul(logits, torch.transpose(logits, 0, 1)))\n",
    "                    sense_norm = torch.diagonal(torch.matmul(sf, torch.transpose(sf, 0, 1)))\n",
    "                    norm = torch.multiply(y_norm, sense_norm)\n",
    "                    E = torch.transpose(torch.transpose(E, 0, 2) / norm, 0, 2)\n",
    "\n",
    "                    E_t = torch.transpose(E, 1, 2)\n",
    "                    E_o = torch.einsum('aij, ajh -> aih', E, E_t)\n",
    "                    E_o = torch.sum(E_o)\n",
    "                    batch_ortho_loss = (self.ortho_ * E_o) / self.batch_size\n",
    "\n",
    "                    batch_sparse_loss = (self.sparse_ * torch.sum(torch.linalg.norm(E, ord = 1, axis = 0))) / self.batch_size\n",
    "                        \n",
    "                    ortho_loss += batch_ortho_loss\n",
    "                    sparse_loss += batch_sparse_loss\n",
    "                    \n",
    "                    start_idx = end_idx\n",
    "                    \n",
    "                loss = self.alpha * process.mi_loss_jsd(res[0], res[1]) +\\\n",
    "                       self.beta * process.mi_loss_jsd(res[2], res[3]) +\\\n",
    "                       self.gamma * process.reconstruct_loss(res[4], adj_target) +\\\n",
    "                       ortho_loss +\\\n",
    "                       sparse_loss\n",
    "            else:\n",
    "                loss = self.alpha * process.mi_loss_jsd(res[0], res[1]) +\\\n",
    "                       self.beta * process.mi_loss_jsd(res[2], res[3]) +\\\n",
    "                       self.gamma * process.reconstruct_loss(res[4], adj_target)\n",
    "\n",
    "\n",
    "            if self.debug:\n",
    "                print('Loss:', loss)\n",
    "\n",
    "            if loss < best:\n",
    "                best = loss\n",
    "                best_t = epoch\n",
    "                cnt_wait = 0\n",
    "                torch.save(model.state_dict(), self.model_name + '.pkl')\n",
    "            else:\n",
    "                cnt_wait += 1\n",
    "\n",
    "            if cnt_wait == self.epoch_flag:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "        self.time_per_epoch = (time.time() - start_time) / epoch\n",
    "\n",
    "        if self.debug: \n",
    "            print('Loading {}th epoch'.format(best_t))\n",
    "            \n",
    "        model.load_state_dict(torch.load(self.model_name + '.pkl'))\n",
    "\n",
    "        embeds = model.embed(features, sp_adj)\n",
    "        self.embeddings = embeds\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        return np.squeeze(self.embeddings.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2f389d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Personalized Page Rank...                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "986it [00:28, 34.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done                                                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zohairshafi/miniforge3/lib/python3.10/site-packages/networkx/algorithms/centrality/katz.py:325: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A = nx.adjacency_matrix(G, nodelist=nodelist, weight=weight).todense().T\n"
     ]
    }
   ],
   "source": [
    "with open('./data/email.pkl', 'rb') as file: \n",
    "    graph_dict = pkl.load(file)\n",
    "    \n",
    "graph = nx.Graph(nx.to_numpy_array(graph_dict['graph']))    \n",
    "graph = nx.Graph(nx.to_numpy_array(graph))\n",
    "\n",
    "\n",
    "sense_feat_dict, sense_features = get_sense_features(graph, ppr_flag = 'std')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cebadf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncorrelated_feats = ['Degree',\n",
    "                    'Clustering Coefficient',\n",
    "                    'Personalized Page Rank - Standard Deviation',\n",
    "                    'Average Neighbor Degree',\n",
    "                    'Average Neighbor Clustering',\n",
    "                    'Eccentricity',\n",
    "                    'Katz Centrality']\n",
    "sense_features = sense_features[:, [list(sense_feat_dict).index(feat) for feat in uncorrelated_feats]]\n",
    "sense_feat_dict = {feat : idx for idx, feat in enumerate(uncorrelated_feats)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a040056b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:22<00:00, 22.03it/s]\n",
      "  0%|                                                   | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m error_og \u001b[38;5;241m=\u001b[39m sense_features \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog((sense_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-10\u001b[39m) \u001b[38;5;241m/\u001b[39m ((embed_og \u001b[38;5;241m@\u001b[39m feature_dict_og[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplain_norm\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-10\u001b[39m)) \u001b[38;5;241m-\u001b[39m sense_features \u001b[38;5;241m+\u001b[39m (embed_og \u001b[38;5;241m@\u001b[39m feature_dict_og[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplain_norm\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m explain_og \u001b[38;5;241m=\u001b[39m (explain_og \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(explain_og)) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mptp(explain_og)\n\u001b[0;32m---> 22\u001b[0m gmi_plus \u001b[38;5;241m=\u001b[39m \u001b[43mGMIEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m           \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m           \u001b[49m\u001b[43mfeature_matrix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msense_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m           \u001b[49m\u001b[43muse_xm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m           \u001b[49m\u001b[43mortho_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m           \u001b[49m\u001b[43msparse_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m embed_plus \u001b[38;5;241m=\u001b[39m gmi_plus\u001b[38;5;241m.\u001b[39mget_embedding()\n\u001b[1;32m     30\u001b[0m embed_plus \u001b[38;5;241m=\u001b[39m (embed_plus \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(embed_plus)) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mptp(embed_plus)\n",
      "Cell \u001b[0;32mIn [24], line 32\u001b[0m, in \u001b[0;36mGMIEmbedding.__init__\u001b[0;34m(self, embed_dim, graph, feature_matrix, use_xm, debug, batch_size, nb_epochs, patience, ortho_, sparse_, lr, l2_coef, drop_prob, sparse, nonlinearity, alpha, beta, gamma, negative_num, epoch_flag, model_name)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [24], line 123\u001b[0m, in \u001b[0;36mGMIEmbedding.embed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m sense_mat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mij, ik -> ijk\u001b[39m\u001b[38;5;124m'\u001b[39m, embeds_, sf)\n\u001b[1;32m    122\u001b[0m E \u001b[38;5;241m=\u001b[39m sense_mat\n\u001b[0;32m--> 123\u001b[0m y_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiagonal(torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[43mlogits\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtranspose(logits, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m    124\u001b[0m sense_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiagonal(torch\u001b[38;5;241m.\u001b[39mmatmul(sf, torch\u001b[38;5;241m.\u001b[39mtranspose(sf, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m    125\u001b[0m norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultiply(y_norm, sense_norm)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logits' is not defined"
     ]
    }
   ],
   "source": [
    "gmi_og = GMIEmbedding(graph = graph, \n",
    "           embed_dim = 64, \n",
    "           feature_matrix = sense_features, \n",
    "           use_xm = False, \n",
    "           ortho_ = 0, \n",
    "           sparse_ = 0, \n",
    "           batch_size = 1)\n",
    "embed_og = gmi_og.get_embedding()\n",
    "embed_og = (embed_og - np.min(embed_og)) / np.ptp(embed_og)\n",
    "feature_dict_og = find_feature_membership(input_embed = embed_og,\n",
    "                                                    embed_name = 'GMI-SF',\n",
    "                                                    sense_features = sense_features,\n",
    "                                                    sense_feat_dict = sense_feat_dict,\n",
    "                                                    top_k = 8,\n",
    "                                                    solver = 'nmf')\n",
    "\n",
    "explain_og = feature_dict_og['explain_norm']\n",
    "error_og = sense_features * np.log((sense_features + 1e-10) / ((embed_og @ feature_dict_og['explain_norm']) + 1e-10)) - sense_features + (embed_og @ feature_dict_og['explain_norm'])\n",
    "explain_og = (explain_og - np.min(explain_og)) / np.ptp(explain_og)\n",
    "\n",
    "\n",
    "gmi_plus = GMIEmbedding(graph = graph, \n",
    "           embed_dim = 64, \n",
    "           feature_matrix = sense_features, \n",
    "           use_xm = True, \n",
    "           ortho_ = 10, \n",
    "           sparse_ = 1, \n",
    "           batch_size = 1)\n",
    "embed_plus = gmi_plus.get_embedding()\n",
    "embed_plus = (embed_plus - np.min(embed_plus)) / np.ptp(embed_plus)\n",
    "feature_dict_plus = find_feature_membership(input_embed = embed_plus,\n",
    "                                                    embed_name = 'GMI+XM',\n",
    "                                                    sense_features = sense_features,\n",
    "                                                    sense_feat_dict = sense_feat_dict,\n",
    "                                                    top_k = 8,\n",
    "                                                    solver = 'nmf')\n",
    "\n",
    "explain_plus = feature_dict_plus['explain_norm']\n",
    "error_plus = sense_features * np.log((sense_features + 1e-10) / ((embed_plus @ feature_dict_plus['explain_norm']) + 1e-10)) - sense_features + (embed_plus @ feature_dict_plus['explain_norm'])\n",
    "explain_plus = (explain_plus - np.min(explain_plus)) / np.ptp(explain_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59105768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
