{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72f27a9",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "46301356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from scripts.utils import *\n",
    "\n",
    "from GMI_.models import GMI, LogReg\n",
    "from GMI_.utils import process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c32f3f",
   "metadata": {},
   "source": [
    "### Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1114e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbedder:\n",
    "    def __init__(self, graph, embed_shape = (128,)):\n",
    "        self.embed(graph)\n",
    "        self.E = list(graph.edges())\n",
    "        self.graph = graph\n",
    "        self.embed_shape = embed_shape\n",
    "    \n",
    "    def embed(self, graph):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        raise NotImplementedError\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c7379ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMIEmbedding(BaseEmbedder):\n",
    "    def __init__(self, embed_dim = 64, graph = None, feature_matrix = None, use_xm = False, debug = False, batch_size = 1, nb_epochs = 500, patience = 20, ortho_ = 0.1, sparse_ = 0.1, lr = 1e-3, l2_coef = 0.0, drop_prob = 0.0, sparse = True, nonlinearity = 'prelu', alpha = 0.8, beta = 1.0, gamma = 1.0, negative_num = 5, epoch_flag = 20, model_name = 'test'):\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Training Params\n",
    "        self.graph = graph\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.patience = patience\n",
    "        self.lr = lr\n",
    "        self.l2_coef = l2_coef\n",
    "        self.feature_matrix = feature_matrix\n",
    "        self.drop_prob = drop_prob\n",
    "        self.hid_units = embed_dim\n",
    "        self.sparse = sparse\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.use_xm = use_xm\n",
    "        self.ortho_ = ortho_\n",
    "        self.sparse_ = sparse_\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.negative_num = negative_num\n",
    "        self.epoch_flag = epoch_flag\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        self.time_per_epoch = None\n",
    "        \n",
    "        if graph is not None:\n",
    "            self.embed()\n",
    "        else:\n",
    "            self.graph = None\n",
    "    \n",
    "    def embed(self):\n",
    "\n",
    "        ####\n",
    "        if self.feature_matrix is None:\n",
    "            feature_matrix = np.identity(len(self.graph))\n",
    "        else: \n",
    "            feature_matrix = self.feature_matrix\n",
    "\n",
    "        adj_ori = nx.to_scipy_sparse_array(graph)\n",
    "        features = sp.lil_matrix(feature_matrix)\n",
    "        features, _ = process.preprocess_features(features)\n",
    "\n",
    "        nb_nodes = features.shape[0]\n",
    "        ft_size = features.shape[1]\n",
    "        \n",
    "        adj = process.normalize_adj(adj_ori + sp.eye(adj_ori.shape[0]))\n",
    "\n",
    "        if self.sparse:\n",
    "            sp_adj = process.sparse_mx_to_torch_sparse_tensor(adj)\n",
    "        else:\n",
    "            adj = (adj + sp.eye(adj.shape[0])).todense()\n",
    "\n",
    "        features = torch.FloatTensor(features[np.newaxis])\n",
    "        if not self.sparse:\n",
    "            adj = torch.FloatTensor(adj[np.newaxis])\n",
    "\n",
    "        if self.feature_matrix is not None: \n",
    "            sense_features = torch.FloatTensor(self.feature_matrix)\n",
    "\n",
    "        model = GMI(ft_size, self.hid_units, self.nonlinearity)\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr = self.lr, weight_decay = self.l2_coef)\n",
    "        \n",
    "#         if self.use_xm:\n",
    "#              model.load_state_dict(torch.load(self.model_name + '.pkl'))\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "            features = features.cuda()\n",
    "            sp_adj = sp_adj.cuda()\n",
    "            \n",
    "        b_xent = nn.BCEWithLogitsLoss()\n",
    "        xent = nn.CrossEntropyLoss()\n",
    "        cnt_wait = 0\n",
    "        best = 1e9\n",
    "        best_t = 0\n",
    "        \n",
    "        \n",
    "        adj_dense = adj_ori.toarray()\n",
    "        adj_target = adj_dense + np.eye(adj_dense.shape[0])\n",
    "        adj_row_avg = 1.0 / np.sum(adj_dense, axis = 1)\n",
    "        adj_row_avg[np.isnan(adj_row_avg)] = 0.0\n",
    "        adj_row_avg[np.isinf(adj_row_avg)] = 0.0\n",
    "        adj_dense = adj_dense * 1.0\n",
    "        \n",
    "        for i in range(adj_ori.shape[0]):\n",
    "            adj_dense[i] = adj_dense[i] * adj_row_avg[i]\n",
    "        adj_ori = sp.csr_matrix(adj_dense, dtype = np.float32)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for epoch in tqdm(range(self.nb_epochs)):\n",
    "            model.train()\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            res = model(features, adj_ori, self.negative_num, sp_adj, None, None) \n",
    "            \n",
    "            if self.use_xm == True and feature_matrix is not None:\n",
    "                \n",
    "                start_idx = 0\n",
    "                loop = True\n",
    "                \n",
    "                ortho_loss = 0\n",
    "                sparse_loss = 0\n",
    "                xm_batch_size = 128\n",
    "                \n",
    "                sf = sense_features\n",
    "                embeds = model.embed(features, sp_adj)\n",
    "                                \n",
    "                while loop:\n",
    "                    end_idx = start_idx + xm_batch_size\n",
    "                    if end_idx > len(self.graph):\n",
    "                        loop = False\n",
    "                        end_idx = len(self.graph)\n",
    "                                            \n",
    "                    sf = sense_features[start_idx : end_idx]\n",
    "                    embeds_ = torch.squeeze(embeds)[start_idx : end_idx]\n",
    "                    \n",
    "                    \n",
    "                    sense_mat = torch.einsum('ij, ik -> ijk', embeds_, sf)\n",
    "                    E = sense_mat\n",
    "                    y_norm = torch.diagonal(torch.matmul(embeds_, torch.transpose(embeds_, 0, 1)))\n",
    "                    sense_norm = torch.diagonal(torch.matmul(sf, torch.transpose(sf, 0, 1)))\n",
    "                    norm = torch.multiply(y_norm, sense_norm)\n",
    "                    E = torch.transpose(torch.transpose(E, 0, 2) / norm, 0, 2)\n",
    "                    E = (E - torch.amin(E, dim = [-1, -2], keepdim = True)) / (torch.amax(E, dim = [-1, -2], keepdim = True) - torch.amin(E, dim = [-1, -2], keepdim = True))\n",
    "                    \n",
    "                    E_t = torch.transpose(E, 1, 2)\n",
    "                    \n",
    "                    E_o = torch.einsum('aij, ajh -> aih', E, E_t)\n",
    "                    E_o = torch.sum(E_o)\n",
    "                    batch_ortho_loss = (self.ortho_ * E_o) / self.batch_size\n",
    "\n",
    "                    batch_sparse_loss = (self.sparse_ * torch.sum(torch.linalg.norm(E, ord = 1, axis = 0))) / self.batch_size\n",
    "                        \n",
    "                    ortho_loss += batch_ortho_loss\n",
    "                    sparse_loss += batch_sparse_loss\n",
    "                    \n",
    "                    start_idx = end_idx\n",
    "                    \n",
    "                loss = self.alpha * process.mi_loss_jsd(res[0], res[1]) +\\\n",
    "                       self.beta * process.mi_loss_jsd(res[2], res[3]) +\\\n",
    "                       self.gamma * process.reconstruct_loss(res[4], adj_target) +\\\n",
    "                       ortho_loss +\\\n",
    "                       sparse_loss\n",
    "            else:\n",
    "                loss = self.alpha * process.mi_loss_jsd(res[0], res[1]) +\\\n",
    "                       self.beta * process.mi_loss_jsd(res[2], res[3]) +\\\n",
    "                       self.gamma * process.reconstruct_loss(res[4], adj_target)\n",
    "\n",
    "\n",
    "            if self.debug:\n",
    "                print('Loss:', loss)\n",
    "\n",
    "            if loss < best:\n",
    "                best = loss\n",
    "                best_t = epoch\n",
    "                cnt_wait = 0\n",
    "                torch.save(model.state_dict(), self.model_name + '.pkl')\n",
    "            else:\n",
    "                cnt_wait += 1\n",
    "\n",
    "            if cnt_wait == self.epoch_flag:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "        self.time_per_epoch = (time.time() - start_time) / epoch\n",
    "\n",
    "        if self.debug: \n",
    "            print('Loading {}th epoch'.format(best_t))\n",
    "            \n",
    "        model.load_state_dict(torch.load(self.model_name + '.pkl'))\n",
    "\n",
    "        embeds = model.embed(features, sp_adj)\n",
    "        self.embeddings = embeds\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        return np.squeeze(self.embeddings.numpy())\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
