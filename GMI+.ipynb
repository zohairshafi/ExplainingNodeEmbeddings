{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72f27a9",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46301356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from scripts.utils import *\n",
    "\n",
    "from GMI_.models import GMI, LogReg\n",
    "from GMI_.utils import process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c32f3f",
   "metadata": {},
   "source": [
    "### Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1114e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbedder:\n",
    "    def __init__(self, graph, embed_shape = (128,)):\n",
    "        self.embed(graph)\n",
    "        self.E = list(graph.edges())\n",
    "        self.graph = graph\n",
    "        self.embed_shape = embed_shape\n",
    "    \n",
    "    def embed(self, graph):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        raise NotImplementedError\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c7379ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMIEmbedding(BaseEmbedder):\n",
    "    def __init__(self, embed_dim = 64, graph = None, feature_matrix = None, use_xm = False, debug = False, batch_size = 1, nb_epochs = 500, patience = 20, ortho_ = 0.1, sparse_ = 0.1, lr = 1e-3, l2_coef = 0.0, drop_prob = 0.0, sparse = True, nonlinearity = 'prelu', alpha = 0.8, beta = 1.0, gamma = 1.0, negative_num = 5, epoch_flag = 20, model_name = 'test'):\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Training Params\n",
    "        self.graph = graph\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.patience = patience\n",
    "        self.lr = lr\n",
    "        self.l2_coef = l2_coef\n",
    "        self.feature_matrix = feature_matrix\n",
    "        self.drop_prob = drop_prob\n",
    "        self.hid_units = embed_dim\n",
    "        self.sparse = sparse\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.use_xm = use_xm\n",
    "        self.ortho_ = ortho_\n",
    "        self.sparse_ = sparse_\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.negative_num = negative_num\n",
    "        self.epoch_flag = epoch_flag\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        self.time_per_epoch = None\n",
    "        \n",
    "        if graph is not None:\n",
    "            self.embed()\n",
    "        else:\n",
    "            self.graph = None\n",
    "    \n",
    "    def embed(self):\n",
    "\n",
    "        ####\n",
    "        if self.feature_matrix is None:\n",
    "            feature_matrix = np.identity(len(self.graph))\n",
    "        else: \n",
    "            feature_matrix = self.feature_matrix\n",
    "\n",
    "        adj_ori = nx.to_scipy_sparse_array(graph)\n",
    "        features = sp.lil_matrix(feature_matrix)\n",
    "        features, _ = process.preprocess_features(features)\n",
    "\n",
    "        nb_nodes = features.shape[0]\n",
    "        ft_size = features.shape[1]\n",
    "        \n",
    "        adj = process.normalize_adj(adj_ori + sp.eye(adj_ori.shape[0]))\n",
    "\n",
    "        if self.sparse:\n",
    "            sp_adj = process.sparse_mx_to_torch_sparse_tensor(adj)\n",
    "        else:\n",
    "            adj = (adj + sp.eye(adj.shape[0])).todense()\n",
    "\n",
    "        features = torch.FloatTensor(features[np.newaxis])\n",
    "        if not self.sparse:\n",
    "            adj = torch.FloatTensor(adj[np.newaxis])\n",
    "\n",
    "        if self.feature_matrix is not None: \n",
    "            sense_features = torch.FloatTensor(self.feature_matrix)\n",
    "\n",
    "        model = GMI(ft_size, self.hid_units, self.nonlinearity)\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr = self.lr, weight_decay = self.l2_coef)\n",
    "        \n",
    "#         if self.use_xm:\n",
    "#              model.load_state_dict(torch.load(self.model_name + '.pkl'))\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "            features = features.cuda()\n",
    "            sp_adj = sp_adj.cuda()\n",
    "            \n",
    "        b_xent = nn.BCEWithLogitsLoss()\n",
    "        xent = nn.CrossEntropyLoss()\n",
    "        cnt_wait = 0\n",
    "        best = 1e9\n",
    "        best_t = 0\n",
    "        \n",
    "        \n",
    "        adj_dense = adj_ori.toarray()\n",
    "        adj_target = adj_dense + np.eye(adj_dense.shape[0])\n",
    "        adj_row_avg = 1.0 / np.sum(adj_dense, axis = 1)\n",
    "        adj_row_avg[np.isnan(adj_row_avg)] = 0.0\n",
    "        adj_row_avg[np.isinf(adj_row_avg)] = 0.0\n",
    "        adj_dense = adj_dense * 1.0\n",
    "        \n",
    "        for i in range(adj_ori.shape[0]):\n",
    "            adj_dense[i] = adj_dense[i] * adj_row_avg[i]\n",
    "        adj_ori = sp.csr_matrix(adj_dense, dtype = np.float32)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for epoch in tqdm(range(self.nb_epochs)):\n",
    "            model.train()\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            res = model(features, adj_ori, self.negative_num, sp_adj, None, None) \n",
    "            \n",
    "            if self.use_xm == True and feature_matrix is not None:\n",
    "                \n",
    "                start_idx = 0\n",
    "                loop = True\n",
    "                \n",
    "                ortho_loss = 0\n",
    "                sparse_loss = 0\n",
    "                xm_batch_size = 128\n",
    "                \n",
    "                sf = sense_features\n",
    "                embeds = model.embed(features, sp_adj)\n",
    "                                \n",
    "                while loop:\n",
    "                    end_idx = start_idx + xm_batch_size\n",
    "                    if end_idx > len(self.graph):\n",
    "                        loop = False\n",
    "                        end_idx = len(self.graph)\n",
    "                                            \n",
    "                    sf = sense_features[start_idx : end_idx]\n",
    "                    embeds_ = torch.squeeze(embeds)[start_idx : end_idx]\n",
    "                    \n",
    "                    \n",
    "                    sense_mat = torch.einsum('ij, ik -> ijk', embeds_, sf)\n",
    "                    E = sense_mat\n",
    "                    y_norm = torch.diagonal(torch.matmul(embeds_, torch.transpose(embeds_, 0, 1)))\n",
    "                    sense_norm = torch.diagonal(torch.matmul(sf, torch.transpose(sf, 0, 1)))\n",
    "                    norm = torch.multiply(y_norm, sense_norm)\n",
    "                    E = torch.transpose(torch.transpose(E, 0, 2) / norm, 0, 2)\n",
    "                    E = (E - torch.amin(E, dim = [-1, -2], keepdim = True)) / (torch.amax(E, dim = [-1, -2], keepdim = True) - torch.amin(E, dim = [-1, -2], keepdim = True))\n",
    "                    \n",
    "                    E_t = torch.transpose(E, 1, 2)\n",
    "                    \n",
    "                    E_o = torch.einsum('aij, ajh -> aih', E, E_t)\n",
    "                    E_o = torch.sum(E_o)\n",
    "                    batch_ortho_loss = (self.ortho_ * E_o) / self.batch_size\n",
    "\n",
    "                    batch_sparse_loss = (self.sparse_ * torch.sum(torch.linalg.norm(E, ord = 1, axis = 0))) / self.batch_size\n",
    "                        \n",
    "                    ortho_loss += batch_ortho_loss\n",
    "                    sparse_loss += batch_sparse_loss\n",
    "                    \n",
    "                    start_idx = end_idx\n",
    "                    \n",
    "                loss = self.alpha * process.mi_loss_jsd(res[0], res[1]) +\\\n",
    "                       self.beta * process.mi_loss_jsd(res[2], res[3]) +\\\n",
    "                       self.gamma * process.reconstruct_loss(res[4], adj_target) +\\\n",
    "                       ortho_loss +\\\n",
    "                       sparse_loss\n",
    "            else:\n",
    "                loss = self.alpha * process.mi_loss_jsd(res[0], res[1]) +\\\n",
    "                       self.beta * process.mi_loss_jsd(res[2], res[3]) +\\\n",
    "                       self.gamma * process.reconstruct_loss(res[4], adj_target)\n",
    "\n",
    "\n",
    "            if self.debug:\n",
    "                print('Loss:', loss)\n",
    "\n",
    "            if loss < best:\n",
    "                best = loss\n",
    "                best_t = epoch\n",
    "                cnt_wait = 0\n",
    "                torch.save(model.state_dict(), self.model_name + '.pkl')\n",
    "            else:\n",
    "                cnt_wait += 1\n",
    "\n",
    "            if cnt_wait == self.epoch_flag:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "        self.time_per_epoch = (time.time() - start_time) / epoch\n",
    "\n",
    "        if self.debug: \n",
    "            print('Loading {}th epoch'.format(best_t))\n",
    "            \n",
    "        model.load_state_dict(torch.load(self.model_name + '.pkl'))\n",
    "\n",
    "        embeds = model.embed(features, sp_adj)\n",
    "        self.embeddings = embeds\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        return np.squeeze(self.embeddings.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "78d02a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Personalized Page Rank...                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1186it [00:23, 50.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done                                                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zohairshafi/miniforge3/lib/python3.10/site-packages/networkx/algorithms/centrality/katz.py:325: FutureWarning:\n",
      "\n",
      "adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./data/usa_airport.pkl', 'rb') as file: \n",
    "    graph_dict = pkl.load(file)\n",
    "    \n",
    "graph = nx.Graph(nx.to_numpy_array(graph_dict['graph']))    \n",
    "graph = nx.Graph(nx.to_numpy_array(graph))\n",
    "\n",
    "\n",
    "sense_feat_dict, sense_features = get_sense_features(graph, ppr_flag = 'std')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "adf6b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncorrelated_feats = ['Degree',\n",
    "                    'Clustering Coefficient',\n",
    "                    'Personalized Page Rank - Standard Deviation',\n",
    "                    'Average Neighbor Degree',\n",
    "                    'Average Neighbor Clustering',\n",
    "                    'Eccentricity',\n",
    "                    'Katz Centrality']\n",
    "sense_features = sense_features[:, [list(sense_feat_dict).index(feat) for feat in uncorrelated_feats]]\n",
    "sense_feat_dict = {feat : idx for idx, feat in enumerate(uncorrelated_feats)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44dc280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:48<00:00, 10.22it/s]\n",
      " 16%|██████▌                                   | 78/500 [00:12<01:08,  6.17it/s]"
     ]
    }
   ],
   "source": [
    "gmi_og = GMIEmbedding(graph = graph, \n",
    "           embed_dim = 128, \n",
    "           feature_matrix = sense_features, \n",
    "           use_xm = False, \n",
    "           ortho_ = 0, \n",
    "           sparse_ = 0, \n",
    "           batch_size = 1)\n",
    "embed_og = gmi_og.get_embedding()\n",
    "embed_og = (embed_og - np.min(embed_og)) / np.ptp(embed_og)\n",
    "feature_dict_og = find_feature_membership(input_embed = embed_og,\n",
    "                                                    embed_name = 'GMI-SF',\n",
    "                                                    sense_features = sense_features,\n",
    "                                                    sense_feat_dict = sense_feat_dict,\n",
    "                                                    top_k = 8,\n",
    "                                                    solver = 'nmf')\n",
    "\n",
    "explain_og = feature_dict_og['explain_norm']\n",
    "error_og = sense_features * np.log((sense_features + 1e-10) / ((embed_og @ feature_dict_og['explain_norm']) + 1e-10)) - sense_features + (embed_og @ feature_dict_og['explain_norm'])\n",
    "explain_og = (explain_og - np.min(explain_og)) / np.ptp(explain_og)\n",
    "\n",
    "\n",
    "gmi_plus = GMIEmbedding(graph = graph, \n",
    "           embed_dim = 128, \n",
    "           feature_matrix = sense_features, \n",
    "           use_xm = True, \n",
    "           alpha = 0.04, \n",
    "           beta = 0.5, \n",
    "           gamma = 0.5,\n",
    "           ortho_ = 1e-2, \n",
    "           sparse_ = 1e-1, \n",
    "           batch_size = 1, \n",
    "           epoch_flag = 500)\n",
    "embed_plus = gmi_plus.get_embedding()\n",
    "embed_plus = (embed_plus - np.min(embed_plus)) / np.ptp(embed_plus)\n",
    "feature_dict_plus = find_feature_membership(input_embed = embed_plus,\n",
    "                                                    embed_name = 'GMI+XM',\n",
    "                                                    sense_features = sense_features,\n",
    "                                                    sense_feat_dict = sense_feat_dict,\n",
    "                                                    top_k = 8,\n",
    "                                                    solver = 'nmf')\n",
    "\n",
    "explain_plus = feature_dict_plus['explain_norm']\n",
    "error_plus = sense_features * np.log((sense_features + 1e-10) / ((embed_plus @ feature_dict_plus['explain_norm']) + 1e-10)) - sense_features + (embed_plus @ feature_dict_plus['explain_norm'])\n",
    "explain_plus = (explain_plus - np.min(explain_plus)) / np.ptp(explain_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1462e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Heatmap(z = explain_og,\n",
    "                         x = list(sense_feat_dict), \n",
    "                         ))\n",
    "fig.update_layout(title_text = 'DGI : ' + str(np.linalg.norm(explain_og, ord = 'nuc')), \n",
    "                  xaxis_title_text = 'Sense Features', \n",
    "                  yaxis_title_text = 'Dimensions')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Heatmap(z = explain_plus,\n",
    "                         x = list(sense_feat_dict), \n",
    "                         ))\n",
    "fig.update_layout(title_text = 'DGI+ : ' + str(np.linalg.norm(explain_plus, ord = 'nuc')), \n",
    "                  xaxis_title_text = 'Sense Features', \n",
    "                  yaxis_title_text = 'Dimensions')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274c6f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_plus = embed_plus\n",
    "sense_mat = tf.einsum('ij, ik -> ijk', Y_plus, sense_features)\n",
    "Y_plus_norm = tf.linalg.diag_part(tf.matmul(Y_plus, Y_plus, transpose_b = True), k = 0)\n",
    "sense_norm = tf.linalg.diag_part(tf.matmul(sense_features, sense_features, transpose_b = True), k = 0)\n",
    "norm = Y_plus_norm * tf.cast(sense_norm, tf.float32)\n",
    "D_plus = tf.transpose(tf.transpose(sense_mat) / norm)\n",
    "D_plus = (D_plus - tf.reshape(tf.reduce_min(D_plus, axis = [-1, -2]), (-1, 1, 1))) / tf.reshape(tf.reduce_max(D_plus, axis = [-1, -2]) - tf.reduce_min(D_plus, axis = [-1, -2]), (-1, 1, 1))\n",
    "\n",
    "\n",
    "Y_og = embed_og\n",
    "\n",
    "sense_mat = tf.einsum('ij, ik -> ijk', Y_og, sense_features)\n",
    "Y_og_norm = tf.linalg.diag_part(tf.matmul(Y_og, Y_og, transpose_b = True), k = 0)\n",
    "sense_norm = tf.linalg.diag_part(tf.matmul(sense_features, sense_features, transpose_b = True), k = 0)\n",
    "norm = Y_og_norm * tf.cast(sense_norm, tf.float32)\n",
    "D_og = tf.transpose(tf.transpose(sense_mat) / norm)\n",
    "D_og = (D_og - tf.reshape(tf.reduce_min(D_og, axis = [-1, -2]), (-1, 1, 1))) / tf.reshape(tf.reduce_max(D_og, axis = [-1, -2]) - tf.reduce_min(D_og, axis = [-1, -2]), (-1, 1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_og = [np.linalg.norm(D_og[node, :, :], ord = 'nuc') for node in tqdm(range(len(graph)))]\n",
    "norm_plus = [np.linalg.norm(D_plus[node, :, :], ord = 'nuc') for node in tqdm(range(len(graph)))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07611813",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = np.array(norm_og) - np.array(norm_plus)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x = diff))\n",
    "fig.update_layout(title_text = 'Distribution of Difference In Nuclear Norms - GMI vs GMI+XM', \n",
    "                  xaxis_title_text = 'Difference In Nuclear Norms - GMI vs GMI+XM', \n",
    "                  yaxis_title_text = 'Frequency', \n",
    "                  plot_bgcolor = 'white', \n",
    "                  paper_bgcolor = 'white', \n",
    "                  font = dict(size = 30))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(x = norm_og, \n",
    "                           name = 'GMI'))\n",
    "fig.add_trace(go.Histogram(x = norm_plus, \n",
    "                           name = 'GMI+XM'))\n",
    "\n",
    "\n",
    "fig.update_layout(title_text = 'Distribution of Nuclear Norm of Node Explain Matrix', \n",
    "                  xaxis_title_text = 'Nuclear Norm', \n",
    "                  yaxis_title_text = 'Frequency', \n",
    "                  paper_bgcolor = 'white', \n",
    "                  plot_bgcolor = 'white', \n",
    "                  font = dict(size = 20))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aa49b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605bcbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac60f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34601e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a1ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
